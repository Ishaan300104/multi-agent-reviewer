{
  "evaluation_date": "2025-11-25T22:24:27.766975",
  "test_results": [
    {
      "test_id": "test_1",
      "name": "High Quality ML Paper",
      "timestamp": "2025-11-25T22:23:59.770970",
      "passed": false,
      "processing_time": 2.26004,
      "review_result": {
        "session_id": "5f361e33-c792-4849-a355-e11e3a9e8d04",
        "paper_content": {
          "paper_id": "paper-providedproperattrib-524b7d7a",
          "title": "Provided proper attribution is provided, Google hereby grants permission to",
          "authors": [
            "Attention Is All You Need",
            "Ashish Vaswani\u2217",
            "Google Brain",
            "Noam Shazeer\u2217",
            "Google Brain",
            "Niki Parmar\u2217",
            "Google Research",
            "Jakob Uszkoreit\u2217",
            "Google Research",
            "Llion Jones\u2217"
          ],
          "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n1",
          "sections": [
            {
              "heading": "1\nIntroduction",
              "level": 1,
              "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks "
            },
            {
              "heading": "2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU",
              "level": 1,
              "content": "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehens"
            },
            {
              "heading": "3\nModel Architecture",
              "level": 1,
              "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forw"
            },
            {
              "heading": "5\nTraining",
              "level": 1,
              "content": "This section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbot"
            },
            {
              "heading": "6\nResults",
              "level": 1,
              "content": "6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute "
            },
            {
              "heading": "7\nConclusion",
              "level": 1,
              "content": "In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to trai"
            },
            {
              "heading": "12\nAttention Visualizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince",
              "level": 1,
              "content": "2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color."
            },
            {
              "heading": "13\nThe\nLaw\nwill\nnever\nbe\nperfect",
              "level": 1,
              "content": ",\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word."
            },
            {
              "heading": "14\nThe\nLaw\nwill\nnever\nbe\nperfect",
              "level": 1,
              "content": ",\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15"
            },
            {
              "heading": "BLEU",
              "level": 1,
              "content": "Training Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 \u00b7 1020\nGNMT + RL [38]\n24.6\n39.92\n2.3 \u00b7 1019\n1.4 \u00b7 1020\nConvS2S [9]\n25.16\n40.46\n9.6 \u00b7 1018\n1.5 \u00b7 1020\nMoE [32]\n26.03\n40.56\n2.0 \u00b7 1019\n1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]\n40.4\n8.0 \u00b7 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 \u00b7 1020\n1.1 \u00b7 1021\nConvS2S Ensemble [9]\n26.36\n41.29\n7.7 \u00b7 1019\n1.2 \u00b7 1021\nTransformer (base model)\n27.3\n38.1\n3.3 \u00b7 1018\nTransformer (big)\n28.4\n41.8\n2.3 \u00b7 1019\nResidual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing\nDuring training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMach"
            },
            {
              "heading": "PPL\nBLEU",
              "level": 1,
              "content": "params\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key"
            },
            {
              "heading": "Attention Is All You Need",
              "level": 2,
              "content": "Ashish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com"
            },
            {
              "heading": "Abstract",
              "level": 2,
              "content": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transf"
            },
            {
              "heading": "Introduction",
              "level": 2,
              "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks "
            },
            {
              "heading": "Background",
              "level": 2,
              "content": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the seque"
            }
          ],
          "references": [
            "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.",
            "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.",
            "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10",
            "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.",
            "Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.",
            "Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.",
            "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.",
            "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.",
            "Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.",
            "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.",
            "Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.",
            "Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.",
            "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.",
            "\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.",
            "\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.",
            "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,",
            "Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.",
            "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.",
            "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.",
            "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.",
            "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11",
            "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.",
            "David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.",
            "Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.",
            "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.",
            "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July",
            "Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.",
            "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.",
            "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.",
            "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.",
            "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.",
            "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.",
            "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.",
            "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.",
            "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.",
            "Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.",
            "Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12\nAttention Visualizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15"
          ],
          "metadata": {
            "source": "data/sample_papers/ml_paper.pdf",
            "page_count": 15,
            "word_count": 6095,
            "extracted_at": "2025-11-25T22:23:59.959880"
          }
        },
        "critic_analysis": {
          "overall_score": 6.675,
          "strengths": [
            "Strong clarity: well-executed and clearly presented",
            "Comprehensive literature review with extensive citations",
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "Minor presentation improvements needed"
          ],
          "detailed_scores": {
            "novelty": 7.0,
            "methodology": 5.3,
            "clarity": 7.7,
            "reproducibility": 6.7
          },
          "recommendations": [
            "Consider adding more ablation studies",
            "Expand discussion of limitations"
          ],
          "methodology_analysis": {
            "has_methodology_section": false,
            "methodology_length": 0,
            "experimental_design": "unclear"
          },
          "clarity_metrics": {
            "abstract_length": 323,
            "section_count": 15,
            "reference_count": 40,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 40,
          "key_citations": [
            {
              "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.",
              "year": 2014,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.",
              "year": 2017,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. ",
              "year": 2014,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.",
              "year": 2014,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.",
              "year": 2017,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.",
              "year": 2013,
              "relevance": "medium",
              "type": "unknown"
            }
          ],
          "related_papers": [
            {
              "arxiv_id": "2106.07104v1",
              "title": "The link between Bitcoin and Google Trends attention",
              "authors": [
                "Nektarios Aslanidis",
                "Aurelio F. Bariviera",
                "\u00d3scar G. L\u00f3pez"
              ],
              "abstract": "This paper shows that Bitcoin is not correlated to a general uncertainty index as measured by the Google Trends data of Castelnuovo and Tran (2017). Instead, Bitcoin is linked to a Google Trends attention measure specific for the cryptocurrency market. First, we find a bidirectional relationship bet",
              "published": "2021-06-13T22:41:55+00:00",
              "similarity_score": 0.1411111111111111,
              "url": "http://arxiv.org/abs/2106.07104v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "2210.09377v1",
              "title": "6th Place Solution to Google Universal Image Embedding",
              "authors": [
                "S. Gkelios",
                "A. Kastellos",
                "S. Chatzichristofis"
              ],
              "abstract": "This paper presents the 6th place solution to the Google Universal Image Embedding competition on Kaggle. Our approach is based on the CLIP architecture, a powerful pre-trained model used to learn visual representation from natural language supervision. We also utilized the SubCenter ArcFace loss wi",
              "published": "2022-10-17T19:19:46+00:00",
              "similarity_score": 0.11263157894736842,
              "url": "http://arxiv.org/abs/2210.09377v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1803.10916v1",
              "title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
              "authors": [
                "Changhao Shan",
                "Junbo Zhang",
                "Yujun Wang"
              ],
              "abstract": "In this paper, we propose an attention-based end-to-end neural approach for small-footprint keyword spotting (KWS), which aims to simplify the pipelines of building a production-quality KWS system. Our model consists of an encoder and an attention mechanism. The encoder transforms the input signal i",
              "published": "2018-03-29T03:32:59+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1803.10916v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1902.08746v1",
              "title": "Can Google Scholar and Mendeley help to assess the scholarly impacts of dissertations?",
              "authors": [
                "Kayvan Kousha",
                "Mike Thelwall"
              ],
              "abstract": "Dissertations can be the single most important scholarly outputs of junior researchers. Whilst sets of journal articles are often evaluated with the help of citation counts from the Web of Science or Scopus, these do not index dissertations and so their impact is hard to assess. In response, this ar",
              "published": "2019-02-23T06:35:12+00:00",
              "similarity_score": 0.09878542510121457,
              "url": "http://arxiv.org/abs/1902.08746v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1301.1290v1",
              "title": "Moving dunes on the Google Earth",
              "authors": [
                "Amelia Carolina Sparavigna"
              ],
              "abstract": "Several methods exist for surveying the dunes and estimate their migration rate. Among methods suitable for the macroscopic scale, the use of the satellite images available on Google Earth is a convenient resource, in particular because of its time series. Some examples of the use of this feature of",
              "published": "2013-01-04T18:31:43+00:00",
              "similarity_score": 0.08263157894736842,
              "url": "http://arxiv.org/abs/1301.1290v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1212.0638v2",
              "title": "Manipulating Google Scholar Citations and Google Scholar Metrics: simple, easy and tempting",
              "authors": [
                "Emilio Delgado Lopez-Cozar",
                "Nicolas Robinson-Garcia",
                "Daniel Torres-Salinas"
              ],
              "abstract": "The launch of Google Scholar Citations and Google Scholar Metrics may provoke a revolution in the research evaluation field as it places within every researchers reach tools that allow bibliometric measuring. In order to alert the research community over how easily one can manipulate the data and bi",
              "published": "2012-12-04T08:30:13+00:00",
              "similarity_score": 0.08263157894736842,
              "url": "http://arxiv.org/abs/1212.0638v2",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1003.3242v3",
              "title": "Private Information Disclosure from Web Searches. (The case of Google Web History)",
              "authors": [
                "Claude Castelluccia",
                "Emiliano De Cristofaro",
                "Daniele Perito"
              ],
              "abstract": "As the amount of personal information stored at remote service providers increases, so does the danger of data theft. When connections to remote services are made in the clear and authenticated sessions are kept using HTTP cookies, data theft becomes extremely easy to achieve. In this paper, we stud",
              "published": "2010-03-16T20:23:59+00:00",
              "similarity_score": 0.07990430622009569,
              "url": "http://arxiv.org/abs/1003.3242v3",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "2310.10159v1",
              "title": "Joint Music and Language Attention Models for Zero-shot Music Tagging",
              "authors": [
                "Xingjian Du",
                "Zhesong Yu",
                "Jiaju Lin"
              ],
              "abstract": "Music tagging is a task to predict the tags of music recordings. However, previous music tagging research primarily focuses on close-set music tagging tasks which can not be generalized to new tags. In this work, we propose a zero-shot music tagging system modeled by a joint music and language atten",
              "published": "2023-10-16T08:00:16+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2310.10159v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2210.12753v3",
              "title": "Google's Quantum Supremacy Claim: Data, Documentation, and Discussion",
              "authors": [
                "Gil Kalai",
                "Yosef Rinott",
                "Tomer Shoham"
              ],
              "abstract": "In October 2019, Nature published a paper describing an experiment that took place at Google. The paper claims to demonstrate quantum (computational) supremacy on a 53-qubit quantum computer. Since September 2019 we have been involved in a long-term project to study various statistical aspects of th",
              "published": "2022-10-23T15:36:25+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2210.12753v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2009.10633v1",
              "title": "ThingML+ Augmenting Model-Driven Software Engineering for the Internet of Things with Machine Learning",
              "authors": [
                "Armin Moin",
                "Stephan R\u00f6ssler",
                "Stephan G\u00fcnnemann"
              ],
              "abstract": "In this paper, we present the current position of the research project ML-Quadrat, which aims to extend the methodology, modeling language and tool support of ThingML - an open source modeling tool for IoT/CPS - to address Machine Learning needs for the IoT applications. Currently, ThingML offers a ",
              "published": "2020-09-22T15:45:45+00:00",
              "similarity_score": 0.0,
              "url": "http://arxiv.org/abs/2009.10633v1",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "The paper cites 40 references, indicating a solid grounding in related work. 0 references are from recent work (2020+), with opportunity to include more recent work.",
          "citation_network": {
            "total_citations": 40,
            "related_papers_found": 10,
            "network_density": 0.25,
            "avg_similarity": 0.08140698482803746
          },
          "topics": [
            "Machine Learning",
            "Computer Vision",
            "Natural Language"
          ]
        },
        "final_review": {
          "executive_summary": "**Provided proper attribution is provided, Google hereby grants permission to**\n\n**Quick Assessment**: Overall Score 6.7/10\n\n**Main Contribution**: The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder.\n\n**Verdict**: This paper presents a moderate contribution to the field.",
          "detailed_review": "# Detailed Review: Provided proper attribution is provided, Google hereby grants permission to\n\n## Overview\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being mor...\n\n## Assessment Scores\n- **Novelty**: 7.0/10\n- **Methodology**: 5.3/10\n- **Clarity**: 7.7/10\n- **Reproducibility**: 6.7/10\n\n## Strengths\n1. Strong clarity: well-executed and clearly presented\n2. Comprehensive literature review with extensive citations\n3. Well-structured paper with detailed sections\n4. Detailed abstract providing good overview\n\n## Weaknesses\n1. Minor presentation improvements needed\n\n## Recommendations for Improvement\n1. Consider adding more ablation studies\n2. Expand discussion of limitations\n\n## Related Work\n- The link between Bitcoin and Google Trends attention (Similarity: 0.14)\n- 6th Place Solution to Google Universal Image Embedding (Similarity: 0.11)\n- Attention-based End-to-End Models for Small-Footprint Keyword Spotting (Similarity: 0.11)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. Provided proper attribution is provided, Google hereby grants permission to is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work adds to our understanding of the problem, though there's still room for improvement.\n\n**The bottom line:**\nThe main good thing: Strong clarity: well-executed and clearly presented\nSomething to improve: Minor presentation improvements needed",
          "key_takeaways": [
            "Main focus: Provided proper attribution is provided, Google hereby grants permission to",
            "Key strength: Strong clarity: well-executed and clearly presented",
            "Strongest aspect: clarity (7.7/10)",
            "Area for improvement: Minor presentation improvements needed",
            "Overall quality: 6.7/10 - Good"
          ],
          "recommendation": "Weak Accept - Good work but needs improvements",
          "confidence": 0.86,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                7.0,
                5.3,
                7.7,
                6.7
              ]
            },
            "summary_card": {
              "title": "Provided proper attribution is provided, Google hereby grants permission to",
              "overall_score": 6.675,
              "recommendation": "Weak Accept - Good work but needs improvements"
            },
            "metrics": {
              "sections": 15,
              "references": 40,
              "word_count": 6095
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 2.259621,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:23:59.770996",
          "end_time": "2025-11-25T22:24:02.030617"
        }
      },
      "constraint_violations": [],
      "expectations_met": false,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 6.675
      }
    },
    {
      "test_id": "test_2",
      "name": "Poorly Structured Paper",
      "timestamp": "2025-11-25T22:24:02.031859",
      "passed": false,
      "processing_time": 4.4918,
      "review_result": {
        "session_id": "114edbf6-be42-4515-a168-af2b6bee56fe",
        "paper_content": {
          "paper_id": "paper-the-070c41fd",
          "title": "The",
          "authors": [],
          "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nAttention Visualizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7\nConclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\n\u03f5ls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3\nEnglish Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2",
          "sections": [
            {
              "heading": "14\nAttention Visualizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince",
              "level": 1,
              "content": "2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Tech"
            },
            {
              "heading": "7\nConclusion",
              "level": 1,
              "content": "In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to trai"
            },
            {
              "heading": "6\nResults",
              "level": 1,
              "content": "6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute "
            },
            {
              "heading": "5\nTraining",
              "level": 1,
              "content": "This section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbot"
            },
            {
              "heading": "1\nIntroduction",
              "level": 1,
              "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks "
            },
            {
              "heading": "2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU",
              "level": 1,
              "content": "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehens"
            },
            {
              "heading": "3\nModel Architecture",
              "level": 1,
              "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2"
            },
            {
              "heading": "PPL\nBLEU",
              "level": 1,
              "content": "params\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key"
            },
            {
              "heading": "BLEU",
              "level": 1,
              "content": "Training Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 \u00b7 1020\nGNMT + RL [38]\n24.6\n39.92\n2.3 \u00b7 1019\n1.4 \u00b7 1020\nConvS2S [9]\n25.16\n40.46\n9.6 \u00b7 1018\n1.5 \u00b7 1020\nMoE [32]\n26.03\n40.56\n2.0 \u00b7 1019\n1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]\n40.4\n8.0 \u00b7 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 \u00b7 1020\n1.1 \u00b7 1021\nConvS2S Ensemble [9]\n26.36\n41.29\n7.7 \u00b7 1019\n1.2 \u00b7 1021\nTransformer (base model)\n27.3\n38.1\n3.3 \u00b7 1018\nTransformer (big)\n28.4\n41.8\n2.3 \u00b7 1019\nResidual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing\nDuring training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMach"
            },
            {
              "heading": "The",
              "level": 2,
              "content": "Law\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
            },
            {
              "heading": "The",
              "level": 2,
              "content": "Law\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
            },
            {
              "heading": "The",
              "level": 2,
              "content": "Law\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works."
            },
            {
              "heading": "Attention Is All You Need",
              "level": 2,
              "content": "Ashish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com"
            },
            {
              "heading": "Abstract",
              "level": 2,
              "content": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transf"
            },
            {
              "heading": "The",
              "level": 2,
              "content": "Law\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
            }
          ],
          "references": [
            "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.",
            "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.",
            "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\n\u03f5ls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64",
            "7\n(E)\npositional embedding instead of sinusoids",
            "4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.",
            "3\nEnglish Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences",
            ". We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]",
            "75\nDeep-Att + PosUnk [39]",
            "0 \u00b7 1020\nGNMT + RL [38]",
            "2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]",
            "0 \u00b7 1020\nGNMT + RL Ensemble [38]",
            "1 \u00b7 1021\nConvS2S Ensemble [9]",
            "2 \u00b7 1021\nTransformer (base model)",
            "3 \u00b7 1018\nTransformer (big)",
            "3 \u00b7 1019\nResidual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing\nDuring training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults",
            "1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.",
            "2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece",
            "and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.",
            "1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.",
            "2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).",
            "3\nOptimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.",
            "4\nRegularization\nWe employ three types of regularization during training:\n7\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 \u00b7 d)\nO(1)\nO(1)\nRecurrent\nO(n \u00b7 d2)\nO(n)\nO(n)\nConvolutional\nO(k \u00b7 n \u00b7 d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r \u00b7 n \u00b7 d)\nO(1)\nO(n/r)",
            "5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n\u2208Rdmodel\u00d7dk, W K\ni\n\u2208Rdmodel\u00d7dk, W V\ni\n\u2208Rdmodel\u00d7dv\nand W O \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.",
            "2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.",
            "3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.",
            "4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.",
            "2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221adk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n\u221adk .",
            "2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.",
            "1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.",
            "2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU",
            ", ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive",
            ", consuming the previously generated symbols as additional input when generating the next.\n2"
          ],
          "metadata": {
            "source": "data/sample_papers/poor_structure.pdf",
            "page_count": 15,
            "word_count": 6095,
            "extracted_at": "2025-11-25T22:24:02.213945"
          }
        },
        "critic_analysis": {
          "overall_score": 6.675,
          "strengths": [
            "Strong clarity: well-executed and clearly presented",
            "Comprehensive literature review with extensive citations",
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "Minor presentation improvements needed"
          ],
          "detailed_scores": {
            "novelty": 7.0,
            "methodology": 5.3,
            "clarity": 7.7,
            "reproducibility": 6.7
          },
          "recommendations": [
            "Consider adding more ablation studies",
            "Expand discussion of limitations"
          ],
          "methodology_analysis": {
            "has_methodology_section": false,
            "methodology_length": 0,
            "experimental_design": "unclear"
          },
          "clarity_metrics": {
            "abstract_length": 2477,
            "section_count": 15,
            "reference_count": 33,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 33,
          "key_citations": [
            {
              "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.",
              "year": 2014,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.",
              "year": 2017,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\nTable 3: Variations on the Transformer architecture. Unliste",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "7\n(E)\npositional embedding instead of sinusoids",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "3\nEnglish Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output",
              "year": null,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": ". We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, bot",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "75\nDeep-Att + PosUnk [39]",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "0 \u00b7 1020\nGNMT + RL [38]",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            }
          ],
          "related_papers": [
            {
              "arxiv_id": "1910.00139v1",
              "title": "Interrogating the Explanatory Power of Attention in Neural Machine Translation",
              "authors": [
                "Pooya Moradi",
                "Nishant Kambhatla",
                "Anoop Sarkar"
              ],
              "abstract": "Attention models have become a crucial component in neural machine translation (NMT). They are often implicitly or explicitly used to justify the model's decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of informatio",
              "published": "2019-09-30T22:30:56+00:00",
              "similarity_score": 0.3633333333333333,
              "url": "http://arxiv.org/abs/1910.00139v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2412.18669v1",
              "title": "Advancing Explainability in Neural Machine Translation: Analytical Metrics for Attention and Alignment Consistency",
              "authors": [
                "Anurag Mishra"
              ],
              "abstract": "Neural Machine Translation (NMT) models have shown remarkable performance but remain largely opaque in their decision making processes. The interpretability of these models, especially their internal attention mechanisms, is critical for building trust and verifying that these systems behave as inte",
              "published": "2024-12-24T20:08:33+00:00",
              "similarity_score": 0.25,
              "url": "http://arxiv.org/abs/2412.18669v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1702.01806v2",
              "title": "Beam Search Strategies for Neural Machine Translation",
              "authors": [
                "Markus Freitag",
                "Yaser Al-Onaizan"
              ],
              "abstract": "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditiona",
              "published": "2017-02-06T22:08:46+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1702.01806v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1509.08644v1",
              "title": "Neural-based machine translation for medical text domain. Based on European Medicines Agency leaflet texts",
              "authors": [
                "Krzysztof Wo\u0142k",
                "Krzysztof Marasek"
              ],
              "abstract": "The quality of machine translation is rapidly evolving. Today one can find several machine translation systems on the web that provide reasonable translations, although the systems are not perfect. In some specific domains, the quality may decrease. A recently proposed approach to this domain is neu",
              "published": "2015-09-29T08:54:48+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1509.08644v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1706.03872v1",
              "title": "Six Challenges for Neural Machine Translation",
              "authors": [
                "Philipp Koehn",
                "Rebecca Knowles"
              ],
              "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",
              "published": "2017-06-12T23:57:48+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1706.03872v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1901.06610v2",
              "title": "Hierarchical Attentional Hybrid Neural Networks for Document Classification",
              "authors": [
                "Jader Abreu",
                "Luis Fred",
                "David Mac\u00eado"
              ],
              "abstract": "Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not tak",
              "published": "2019-01-20T01:48:43+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1901.06610v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1912.02047v2",
              "title": "Neural Machine Translation: A Review and Survey",
              "authors": [
                "Felix Stahlberg"
              ],
              "abstract": "The field of machine translation (MT), the automatic translation of written text from one natural language into another, has experienced a major paradigm shift in recent years. Statistical MT, which mainly relies on various count-based models and which used to dominate MT research for decades, has l",
              "published": "2019-12-04T15:16:03+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1912.02047v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1005.0280v6",
              "title": "Superconductivity as a consequence of an ordering of the electron gas zero-point oscillations",
              "authors": [
                "Boris V. Vasiliev"
              ],
              "abstract": "This paper has been administratively withdrawn by arXiv, duplicate of arXiv:1008.2691.",
              "published": "2010-05-03T13:08:23+00:00",
              "similarity_score": 0.07763157894736841,
              "url": "http://arxiv.org/abs/1005.0280v6",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1304.1836v2",
              "title": "A Simulation and Modeling of Access Points with Definition Language",
              "authors": [
                "Tairen Sun"
              ],
              "abstract": "This submission has been withdrawn by arXiv administrators because it contains fictitious content and was submitted under a pseudonym, which is against arXiv policy.",
              "published": "2013-04-06T00:18:59+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/1304.1836v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1011.5746v2",
              "title": "Intutionistic Fuzzy Ideals in \u0393-semiring",
              "authors": [
                "Nayyar Mehmood",
                "I. H Qureshi"
              ],
              "abstract": "This article has been withdrawn by arXiv administrators due to plagiarized content from arXiv:1010.2469.",
              "published": "2010-11-26T10:21:14+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/1011.5746v2",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "The paper cites 33 references, indicating a solid grounding in related work. 1 references are from recent work (2020+), with opportunity to include more recent work.",
          "citation_network": {
            "total_citations": 33,
            "related_papers_found": 10,
            "network_density": 0.30303030303030304,
            "avg_similarity": 0.1351783625730994
          },
          "topics": [
            "Machine Learning",
            "Computer Vision",
            "Natural Language"
          ]
        },
        "final_review": {
          "executive_summary": "**The**\n\n**Quick Assessment**: Overall Score 6.7/10\n\n**Main Contribution**: The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder.\n\n**Verdict**: This paper presents a moderate contribution to the field.",
          "detailed_review": "# Detailed Review: The\n\n## Overview\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being mor...\n\n## Assessment Scores\n- **Novelty**: 7.0/10\n- **Methodology**: 5.3/10\n- **Clarity**: 7.7/10\n- **Reproducibility**: 6.7/10\n\n## Strengths\n1. Strong clarity: well-executed and clearly presented\n2. Comprehensive literature review with extensive citations\n3. Well-structured paper with detailed sections\n4. Detailed abstract providing good overview\n\n## Weaknesses\n1. Minor presentation improvements needed\n\n## Recommendations for Improvement\n1. Consider adding more ablation studies\n2. Expand discussion of limitations\n\n## Related Work\n- Interrogating the Explanatory Power of Attention in Neural Machine Translation (Similarity: 0.36)\n- Advancing Explainability in Neural Machine Translation: Analytical Metrics for Attention and Alignment Consistency (Similarity: 0.25)\n- Beam Search Strategies for Neural Machine Translation (Similarity: 0.11)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. The is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work adds to our understanding of the problem, though there's still room for improvement.\n\n**The bottom line:**\nThe main good thing: Strong clarity: well-executed and clearly presented\nSomething to improve: Minor presentation improvements needed",
          "key_takeaways": [
            "Main focus: The",
            "Key strength: Strong clarity: well-executed and clearly presented",
            "Strongest aspect: clarity (7.7/10)",
            "Area for improvement: Minor presentation improvements needed",
            "Overall quality: 6.7/10 - Good"
          ],
          "recommendation": "Weak Accept - Good work but needs improvements",
          "confidence": 0.86,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                7.0,
                5.3,
                7.7,
                6.7
              ]
            },
            "summary_card": {
              "title": "The",
              "overall_score": 6.675,
              "recommendation": "Weak Accept - Good work but needs improvements"
            },
            "metrics": {
              "sections": 15,
              "references": 33,
              "word_count": 6095
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 4.491235,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:24:02.031881",
          "end_time": "2025-11-25T22:24:06.523116"
        }
      },
      "constraint_violations": [],
      "expectations_met": false,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 6.675
      }
    },
    {
      "test_id": "test_3",
      "name": "ArXiv Paper by ID",
      "timestamp": "2025-11-25T22:24:06.524219",
      "passed": true,
      "processing_time": 4.649153,
      "review_result": {
        "session_id": "eb1f8703-b9ac-40a5-8b52-3e6e7010abfc",
        "paper_content": {
          "paper_id": "paper-nftrigusingblockchai-cc0485a9",
          "title": "NFTrig: Using Blockchain Technologies for Math Education",
          "authors": [
            "Additional Key Words",
            "Phrases: Matic",
            "Metamask",
            "polygon",
            "bootstrap5",
            "Solidity"
          ],
          "abstract": "NFTrig: Using Blockchain Technologies for Math Education\nJORDAN THOMPSON, Augustana College, USA\nRYAN BENAC, Augustana College, USA\nKIDUS OLANA, Augustana College, USA\nTALHA HASSAN, Augustana College, USA\nANDREW SWARD, Augustana College, USA\nTAUHEED KHAN MOHD, Augustana College, USA\nNFTrig is a web-based application created for use as an educational tool to teach trigonometry and block\nchain technology. Creation of the application includes front and back end development as well as integration\nwith other outside sources including MetaMask and OpenSea. The primary development languages include\nHTML, CSS (Bootstrap 5), and JavaScript as well as Solidity for smart contract creation. The application itself\nis hosted on Moralis utilizing their Web3 API. This technical report describes how the application was created,\nwhat the application requires, and smart contract design with security considerations in mind. The NFTrig\napplication has underwent significant testing and validation prior to and after deployment. Future suggestions\nand recommendations for further development, maintenance, and use in other fields for education are also\ndescribed.\nCCS Concepts: \u2022 Computer systems organization \u2192Redundancy; Robotics; \u2022 Networks \u2192Network\nreliability.\nAdditional Key Words and Phrases: Matic, Metamask, polygon, bootstrap5, Solidity\n1\nINTRODUCTION\nThe purpose of this report is to describe the technical details involved in the development of the\nNFTrig application. This includes both the front end website design, the back end smart contract,\nand NFT creation. It will mainly focus on the technical details of the project outlining software\nrequirements, design through programming languages, client and server side interactions, and\nvalidation testing. This allows the reader to undertake further development, fixes, or maintenance\nof the software, as this forms part of the documentation for the software.\nThe NFTrig project is based around the creation of a web-based game application that allows\ninteraction of NFTs (non-fungible token) with trigonometric function designs. NFts are digital\nassets, for example a picture, that has a unique identification and can generally be freely traded\nwith cryptocurrency [33]. Through this application, users are able to purchase digital artwork of\nmany different trigonometric functions and combine them using mathematical operations. Current\nsupported operations include multiplication and division of the trigonometry functions, and the\noutput of each operation is a new NFT card that would be the result of an operation. The old cards\nwill then be removed from the user\u2019s possession and burned using the smart contact. For example,\nif a user combined the two cards Sin(x) and Cos(x) using multiplication, they would lose their two\nold cards and receive the new card Tan(x). Further, the NFT cards are assigned one of the following\nrarity levels: common, uncommon, rare, and legendary. The probability of each of these levels is\ndefined later in this report.\nThe application also allows a user to connect to MetaMask, a digital wallet capable of storing a\nuser\u2019s cryptocurrency and NFTs as well as a way to connect to block chain. The NFTrig application\nAuthors\u2019 addresses: Jordan Thompson, jordanthompson18@augustana.edu, Augustana College, Rock Island, USA; Ryan\nBenac, ryanbenac18@augustana.edu, Augustana College, Rock Island, USA; Kidus Olana, kidusolana18@augustana.edu,\nAugustana College, Rock Island, USA; Talha Hassan, talhahassan18@augustana.edu, Augustana College, Rock Island,\nUSA; Andrew Sward, andrewsward@augustana.edu, Augustana College, Rock Island, USA; Tauheed Khan Mohd,\ntauheedkhanmohd@augustana.edu, Augustana College, Rock Island, USA.\narXiv:2301.00001v1  [cs.HC]  21 Dec 2022\n2\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\ncan also display the NFTs owned by the user and allow them to connect to OpenSea to sell the\nNFTrig cards on a public marketplace. The application is hosted on Moralis employing their Web3\nAPI. Technical languages used in this project, which will be discussed in detail throughout this\npaper, include front end web development languages HTML, CSS (specifically Bootstrap5), and\nJavaScript as well as the back end smart contract development language Solidity.\nIn order to attract users, this application also allows a user to answer trivia questions and gain\nexperience points. These points can then be used to unlock new sets of NFT cards or upgrade existing\ncards in a user\u2019s wallet. This game-like design should appeal to a younger audience and encourage\nthem to answer trigonometry or math based questions. This will have an incredible educational\nbenefit for the user because they will be both learning and playing a game simultaneously.\n2\nMOTIVATION\nThe purpose of this application is as an educational tool for students who are attempting to\nunderstand the ways that trigonometric functions interact with each other. As opposed to just\ngraphing these functions by hand, students will be able to generate new NFTs by combining\nwhatever trigonometric functions they already own. In fact, using technology is shown to influence\nand better educational processes by increasing interaction between those in the classroom [9].\nTechnology is becoming increasingly prevalent in every sphere of daily life, so the use of technology\nin a classroom setting is not only logical, but it increases the educational benefit of students [29].\nHowever, as the technology continues to evolve, \"the gap between traditional course material\ntaught to students in B.S./M.S. programs at universities and the cutting edge of technology used in\nindustry is widening at an unprecedented rate\" [30]. By creating this project, it will give students\nthe opportunity to gain experience with block chain, and hopefully be a starting place for narrowing\nthat ever growing gap. After much research, it is likely that this proposed application is the first of\nits kind that utilizes NFTs to teach mathematical concepts.\nAside from user benefit of this application, there is also an intellectual merit in the block chain\nand education fields. Best described by Carmen Holotescu, \"As education becomes more open,\ndiversified, democratised, and decentralised, the block chain technology is taken in consideration\nby researchers, teachers and institutions, to maintain reputation, trust in certification, and proof of\nlearning\" [17]. Further, development of this project continues research on NFT and block chain\ntechnologies. This application can also serve as the boilerplate basis for other NFT-based educational\ntools and resources. Research for this project provides opportunities for training computer science\nstudents on how to use NFTs in general, but more specifically in educational contexts.\nNFTrig was developed by computer science students as a final senior inquiry project at Augustana\nCollege. In conjunction and with funding by the Department of Mathematics and Computer Science,\nthis project employs a variety of software development skills and techniques that further the\nresearch and understanding of the block chain and web development field.\n3\nRELATED WORK\nBlock chain technology has enabled the formation of decentralized distributed records of digital\ndata which does not require any third party to moderate any transactions [34]. The decentralized\nnature of block chain also renders it easy for use in a ranging variety of applications in several fields\nsuch as healthcare [16], internet of things [7], gaming [2], banking [6], and education (explored in\ngreater detail in subsection 3.1). Non Fungible token (NFTs) are a relatively new phenomena within\nthe field of block chain based technologies, but its application in aforementioned fields are already\nbeing studied. Specifically within the healthcare context, NFT\u2019s are solving long term issues such as\nstoring patients\u2019 private data more safely as well as maintaining better records while giving better\nautonomy and privacy to both patients and healthcare providers [22]. The application of NFTs in\nNFTrig: Using Blockchain Technologies for Math Education\n3\neducation is still an understudied area. These next related work sections explore the broader use of\nblock chain based technologies for educational purposes, gamification, and overall collaborative\nlearning.\n3.1\nBlock chain Based Technologies for Educational Purposes\nThere has been extensive work concerning how block chain based technologies are enabling better\nownership and sharing of personal records for students and supporting collaborative learning\nenvironments. Yumna et al. conducted a systematic literature review of the use of block chain\ntechnologies in educational sector [35]. They also propose several uses of existing block chain\nbased technologies in educational sector that leverage the decentralized and traceable consensus\nmaking mechanisms of block chain. Researchers have examined the use of block chain to allow\nstudents to maintain educational records such as transcripts, credentials, diplomas, and learning\nactivities [5, 14, 31]. Similarly, research has also explored learning management systems design\nbased on block chain based technology. The technology can potentially verify a students records as\nwell as enable the design of an automatic decentralized enrollment system which does not require\nmoderation from school staff [31].\nAnother elegant use of block chain in the field of education is the ability to support life-long\nlearning applications. The educational sector is becoming more diverse with a variety of different\ntypes of classrooms and learning modalities. E-learning has also allowed students to acquire\nlicences and accreditation online. Therefore, it is imperative to maintain the learning journeys of\nstudents over time to understand the different types of learning that they have been engaging in\nand improving on over time. The traceable nature of block chain based technologies (defined as\none of the salient features in the aforementioned systematic review by [35]) enables all of these\napplications.\nThe decentralized nature of block chains coupled with the consensus making algorithms also\nmakes it suitable for collaborative environments. Prior research has looked at how block chain\nbased technologies can enable better developmental experiences in the realm of business [11] but\nthere is very minimal work on its application within the field of education application[3].\n3.2\nApplications in Education Application and Collaborative Learning\nAlthough preliminary in nature, limited prior work has explored the utilization of NFTs for design-\ning various different independent learning environments for students. There are some proposed\ncommercial systems that have analogous functioning to some of the systems described in the prior\nsection. For example, commercial systems are looking at leveraging NFTs to award \u201cPass\" status\nto students for different courses 1. NFTs enjoy a key advantage over conventional block chain\ntechnologies as they are typically designed using the more secure Ethereum block chain enabling an\neven more secure record and identity management. Researchers have shown that there is promise\nin using NFTs as academic tokens to represent student transcripts and other records as well that\ncan be more easily verified [9]. However, there is still a dearth of academic literature in this field.\nStudent incentivization is heavily advocated in pedagogical literature [12]. NFTs make it easier\nto tie incentivization to learning outcomes as they can be automatically acquired by students at\nany time upon completion of learning outcomes. This gives NFTs based certifications an advantage\nover the more traditional learning settings where students have to strongly adhere to semester\ntimelines. Elmessiry et al. has looked at designing an incentive mechanism that can be used by\nteachers and students to achieve better learning outcomes in an effective and cost-efficient manner\n1A teacher at Pepperdine University using NFTs to award course completion certifications to students: https://upcea.edu/tech-\ntrends-in-higher-ed-metaverse-nft-and-dao/\n4\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\n[9]. They also concluded there was better engagement outcomes for students. On several metrics\nof usability, the students reported more than 80% preference for buying, using, and collecting\nNFTs. Such independent learning methods were particularly more useful during the COVID-19\npandemic to accommodate the need of remote independent learning options. Architecturally, this\nproject takes inspiration from [9], and applies it to a more narrower, focused domain of learning\nmathematical operations in this study. Further, these NFTs are also easier to share on social media\n[20]. Therefore, it also allows students to more readily share their accomplishments.\n3.3\nGamification to Support Mathematical Learning\nSince the proposed application teaches mathematical and trigonometric formulas to students, the\nliterature on use of gamification to support mathematical learning should be better described.\nGamification, in combination with incentivization explained in the previous section, will allow\nfor the success of this application. Gaming settings have traditionally been used to teach simple\nmathematical operations to students. More recently, researchers have also proposed systems that\nteach advanced concepts to students including College Algebra [10]. These learning environments\nmake it easier for students to relate the learning concepts with more daily life phenomena. While\ngamification itself cannot guarantee better learning outcomes, it can improve students\u2019 interest\nand performance by encouraging them to engage with the content for a longer duration of time\n[18]. The simpler, more systematic, and operational nature of mathematics as a subject also makes\nit easier for incorporation in gaming environments because final answers are usually short and\nnumerical as opposed to long and descriptive answer that might be found in social or natural\nsciences. Trigonometry especially can easily be broken down into a series of operations and steps\nwhich simulates a similar environment found in other online games where users play to find\ndifferent \u201crewards\" and \u201ccollectables\". Despite all these benefits there are some limitations of\ngamification as well. For example, it is hard to know how a student arrived a solution and give\nfeedback [4]. Not being able to solve trigonometric equations can also lead to frustration and\nimpeded learning experience. Foresight into the project\u2019s future looks to mitigate these concerns\nby fostering better communication between different game players and providing links to useful\nlearning resources in the application. Prior research has extensively explored the use of gamification\nin different mathematical fields. This application is likely the first to extend the use of NFTs and\nblock chain to aid in teaching trigonometric equations.\nResearch shows that technology, specifically games are shown to be excellent educational tools.\nIn fact, \"one of the most successful positive reinforcement mechanisms [in education] known is\ngamification\" [9]. This includes taking a topic transforming it into a game with positive reinforce-\nment. This leverages educational benefits in students and encourages them to continue playing the\ngame to learn. Nftrig has future plans to add a game function which will allow the user to answer\ntrigonometry trivia and math questions. This will aid in both their learning and the continued use\nof the NFTrig application. Further, the ability to combine owned NFTs with math functions also\naids in the education of trigonometry for the student.\n4\nEXPERIMENTAL SETUP\n4.1\nSoftware Development Requirements\nThe NFTrig application employs a variety of software development requirements that cover the\nrange of the project. From front end web development to back end smart contract creation and\nNFT storage, this section describes the requirements and software used to complete the project.\n4.1.1\nCompiling IDE. The smart contracts created for NFTrig are hosted on Remix. Remix is an\nan open source online compiler IDE that can be used to test and deploy smart contracts [1]. The\nNFTrig: Using Blockchain Technologies for Math Education\n5\nplatform can be accessed by any browser, and it allows the developer to write and deploy smart\ncontracts on an actual or test server simultaneously. The current deployment is on a test server. In\norder to test and debug the smart contract, Visual Studio Code is used. Visual Studio was found\nto be the best code editor because a developer can easily upload most file types, and edit them\n[19]. For NFTrig, it was used to develop front end HTML and CSS files, as well as back end solidity\ncontract editing. The required installed plugins for Visual Studio (VS) include Solidity and Block\nchain development. [21] These allowed for simple, straightforward development of code.\n4.1.2\nMoralis. Moralis SDK is the primary back end platform for the project. The platform allows\nconnection of the front end web application to the smart contract. [8] The Moralis platform uses\na combination of server management and a JavaScript SDK to allow for maximum interaction\nand simplicity. A developer can do many tasks through this including authentication of users,\ngetting necessary user data, and connecting with MetaMask in a non-complicated and simply coded\nprocess. The only expectation is that a developer will need to have programming knowledge in\nJavaScript as well as a familiarity with Moralis and MetaMask, experience querying a database, and\nsome knowledge of Web3 development to ensure maximum results and efficiency. Moralis also has\nthe ability to easily connect to MetaMask.\n4.1.3\nMetaMask. MetaMask is the digital wallet required for participation in the NFTrig game\napplication. It allows the collection of purchases from the user, and it can be installed as an extension\non a browser for increased ease of use [28]. MetaMask stores all NFTs owned by the user, and in\nconnection with the NFTrig application, can view and upgrade or modify existing NFTs at a users\ndiscretion. Connection to the browser extension is required for the application to access anything\nowned by a user [24]. Because MetaMask is easily integrated into Moralis, and thus NFTrig, there\nis little a user needs to do to create a connection aside from installing the MetaMask extension, and\nclicking connect.\n4.1.4\nFront End Design. Front end design was accomplished primarily through Visual Studio. The\nLive Server extension was installed which allows each developer to \"host\" their developed website\nusing a native web application. Doing so allowed simplified testing and front end development.\nInstead of creating CSS files from scratch, the NFTrig interface heavily employs Bootstrap5, which\nsimplifies the process of modifying the content layout and design of buttons and other content\n[25]. Moralis and Bootstrap5 each have extensive documentation to understand and support front\nend web development. These tools have been utilized to a near maximum extent.\n4.1.5\nWeb Hosting Platform. The initial testing of NFTrig, as previously explained, was hosted on\na local live server through Visual Studio. After initial development, the project was moved to a web\nserver hosted by Augustana College so that initial testing could begin. It is currently unclear how\nthe site will ultimately be hosted. One option for hosting the web application is directly through\nGoogle [32]. This would allow the website to be named something easily searchable and accessible.\nA second option would be to host directly through Moralis, but a limitation of this would be a\nmore diluted website naming convention along with a more confusion process of uploading and\nmodifying website content. Currently, the NFTrig application will remain on the local Augustana\nCollege Server.\n5\nSOFTWARE DESIGN\nThis section covers all of the decisions necessary to understand the development of NFTrig, as well\nas the technical implementation of each technology used in the design process.\n6\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\n5.1\nSoftware Architecture\nThe architecture of this project follows the model-server design architecture [27]. Using this model,\nthe clients send transactions and requests to a proxy smart contract stored on the block chain\nwhich then makes the appropriate calls to the logic smart contract which is also stored on the block\nchain. This style of architecture is required for this project because the smart contracts must be\nstored on the server-side chain in order to be functional. The use of proxy contracts also allows our\nsmart contracts to be fully upgradeable with any future updates that may need to be implemented.\n5.2\nChoice of Programming Language\nThis section examines and explains the benefit of each chosen language employed in NFTrig. Front\nend languages include HTML and CSS and the back end includes Solidity and JavaScript. Each has\nbeen chosen because they were found to be the best option for development.\n5.2.1\nSolidity. Solidity is the programming language of choice when it comes to coding smart\ncontracts. Solidity is \"similar to JavaScript and yet has some features of object-oriented languages\nsuch as Java and C++\" [26]. This is a leading language for the development of smart contracts and\nuse on block chain technologies. This project utilizes the solidity library openzeppelin in order to\ncreate a solid foundation for the smart contracts. Hardhat and Node JS are then used for the testing\nand deployment of the smart contracts to the Polygon blockchain.\n5.2.2\nJavaScript. In the NFTrig application, JavaScript (JS) is primarily used in the front end\napplication. The primary purpose of this language is generally to create dynamic and interactive\nweb content [15]. For the client, JS was used in the navigation bar to allow for clickable links and\nresizing of the navigation bar in smaller screens. This language was also used to give buttons\nfunctionality ranging from logging in to MetaMask to purchasing NFTrig cards. Further, JS was used\nto test the logic of the front-end combination page until the smart contract was applied. Aside from\naugmenting HTML and CSS application pages, JavaScript is also used in this project to connect the\nback end smart contract with the from end web application. This application was also developed\nusing Next JS and deployed via an application known as vercel.\n5.2.3\nHTML and CSS. Web development of the user interface was primarily completed using\nHTML and CSS (Bootstrap5). These languages are equally popular and necessary to develop the\nweb pages [13]. Instead of creating all CSS requirements from scratch, Bootstrap5 was utilized to\nallow for cleaner design across web pages and better alignment of web page elements. Bootstrap5\nalso simplifies the need to explicitly code buttons and other interactive items.\n5.3\nSecurity Considerations\nThroughout this project, there have been several security considerations discovered that threatened\nthe safety and use of the application. One such discovered issue was initially, there was no code\nwritten to block a user from looking at another users token. Further, before minting a new NFT\ncard, the smart contracts check to ensure that the card does not already exist, the cards used for\ncombining are owned by the user, and that the newly minted card follows the correct probabilities\nof outcomes shows in 2. These probabilities are coded into the smart contract.\n5.4\nSmart Contract Design\nThe smart contract for this project is broken up into two separate contracts. The first of which is\nthe NFTrig logic contract which contains the logic for purchasing packs of cards as well as the logic\nfor how cards will interact with each other. The second contract is the marketplace contract which\nwill allow users to trade their own NFTs with other users through the website. Within the NFTrig\nNFTrig: Using Blockchain Technologies for Math Education\n7\ncontract, there are functions for multiplying and dividing cards, purchasing randomized packs of\ncards, and tracking the details of each individual token as transactions are made. The marketplace\ncontract contains information about sale history as well as the functionality to post new sales and\npurchase items for sale. Both of these contracts were deployed as upgradeable contracts so they\ncan have updates implemented in the future.\n5.5\nNFT Storage and Naming Conventions\nAll NFT images are stored on the server with the HTML, CSS, and JS files. The naming convention\nfor each image references what image it is in four numbers. The first number is the power of sin, the\nsecond is the power of cos, the third is the rarity or color of the card (0-3 is green, blue, purple, and\nred respectively), and the final number is the text variant (0-3). These files were named accordingly\nto better determine the output if cards were combined using a mathematical function. For example,\na sin card might have the naming convention: 1023.jpg. 10 defines it is a sin card, 2 defines it is\nrarity purple, and 3 defines it is text variant 3. The purpose of naming the files in this way is so\nthat the front end can easily determine which image corresponds to a particular NFT by simply\nlooking at the four features of each token which match the four numbers in the file name.\n5.6\nClient Design\nThe NFTrig application interface was designed using HTML and CSS. The primary use of CSS was\noften replaced by Bootstrap5. Bootstrap 5, a library for CSS, allows for easier scaling and alignment\nof objects in the HTML file, and thus the computer screen [23]. Documentation on the Bootstrap5\nhas utilized to a full extent. Each section examines the layout and use of each application page.\n5.6.1\nNFTrig Home. The interface is designed to allow a user to access the marketplace, their\nindividual current collections, and their profile. The navigational bar contains links to the client-side\nfacing pages: NFTrigHome, MyCards, CombineCards, Marketplace, and Game. We used a total of\nthree colors to enable good contrast and make it easier for our users to view complex graphs and\nformula without a cluttered background 2. The JavaScript elements declared are reusable across\nmultiple screens. They support functions and interactions such as a user hovering over a cell or\nclicking a cell and providing both feedback and error handling to the user. The navigation bar is\nalso, the top bar changes color to indicate the tab that the user is on.\n5.6.2\nCombination. The main purpose of the combination page is for users to choose cards that\nthey currently own, and see options for combining them using either multiplication or division.\nFigure 1 displays the layout of the screen where user selected cards are shown on the left, and\npotential results are shown on the right.\nThe page utilizes Bootstrap5 capabilities to format effectively to different screen sizes and\nresolutions. It connects with a back end script to the smart contract. This provides functionality to\nthe buttons and easy generation of possible NFT results. Below shows the probabilities of generated\nNFT outcomes based on the selected input cards.\n5.6.3\nMarketplace and MyCards. Marketplace and MyCards are similar pages, as they connect to\na data source and display NFTs. The Marketplace tab shows all NFT cards available for purchase\nboth from other users who own NFTs and cards owned by the NFTrig project. MyCards however\nspecifically shows all cards owned by a user. The layout for each generates all necessary NFT\nimages and information about the rarity. The rarity is signified by the color and the text option of\nthe card. Figure 3 shows the actual layout displayed on the page.\n2Background-color:#333, Color: #f2f2f2,\n8\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\nFig. 1. Interface where users will combine NFTrigs\nFig. 2. Probabilities of outcomes depending on rarity of selected cards\n5.6.4\nQuality attributes of client-side interface and code. In order to have an application of quality,\nconsistency, and accuracy, the project followed the following guidelines:\n(1) The code is written in a manner that components and layouts can be rearranged to support\nany structural changes in the front end.\n(2) The code has consistent style and format, such as the padding used in individual NFTrig\nelements and the purchase page\u2019s color.\n(3) The code contains comments and is well indented for easy maintenance and understanding.\n(4) Consistent colors and feedback systems are provided so the system is easy to learn for users.\n(5) Page-level styling was avoided when possible to keep design consistent.\n(6) Thorough testing was completed for basic accessibility features.\n5.6.5\nTesting the Client Design. Basic unit testing of different elements was initially conducted\nto ensure easy navigation between front end pages. In order to ensure that testing would cover\nNFTrig: Using Blockchain Technologies for Math Education\n9\nFig. 3. Interface displaying NFTrig Marketplace\nmost application uses, three user cases were devised: a user browsing NFTrigs, a user making a\npurchase, and a user combining NFTrigs. All assumptions and expected actions expected from\nthe system were listed and analyzed through testing. Further, testing through some edge cases\nwere also pursued. Currently, the application works as intended, however future plans involve\nrigorous testing with JavaScript code and external APIs (if any are devised). This will ensure a fully\nfunctional, secure, and usable application that can also be used as a boiler plate project for other\neducational blockchain technologies.\n5.6.6\nFuture Work: Game. Future work for this project will include the ability for users to play\na trivia and trigonometric equation game. This allows a user to gain experience points that they\ncan then use to purchase new NFTs. This eliminates the need to always need cryptocurrency to\npurchase individual or group NFT cards. Although there is not currently an interface for this page\nwritten in HTML, functionality exists for the trivia game itself. The files are currently stored on\nthe server, but they are disabled and there is no navigable way to get there through the application.\n6\nMETHODS\nMost methods for completing this project have been thoroughly explained in the sections above.\nHowever, the final intended version of this project will be hosted in a different location than it\nresides currently. The initial portion of this project had the front end website hosted on a local\nAugustana College server and the back end smart contract hosted on the Polygon test net. This\nallowed initial testing and validation that the smart contract operated as expected, as well as give\ntime and opportunity to discover security vulnerabilities. The future of this project will be hosted\non a decentralized web application online so that users can access it and begin to interact with the\nsmart contract. Further, a redesign of the website user interface is likely. This will require transition\nfrom BootStrap5 to NextJS which allows cards to be generated, displayed, and interactable through\na version of JavaScript.\n7\nRESULTS\nThis project successfully allowed the exploration and creation of applying NFT and block chain\ntechnology to math education. Although preliminary in use and nature, this project allows for\ninitial project creation as a boiler plate project. The smart contract is currently deployed on the\n10\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\nPolygon testnet and can be interacted with using test Matic. Each web page has functionality to\ndisplay the user\u2019s owned NFTs as well as the NFTs they have put for sale on the marketplace. Using\nNextJS will also allow the Combination page to have functionality and smart contract use. It is also\nworth noting that the created web page is not required to interact with the NFTrig smart contracts.\n8\nRECOMMENDATIONS FOR FUTURE WORK\nThe goal for this project was a working Beta demo that shows application functionality, and correct\nsmart contract execution. There are many other features planned for the continued work of this\nproject. The first, as earlier explained, is a game option which challenges the user with trigonometry\ntrivia and math problems. Answering these questions successfully will increase the experience\npoints of a user. The user can then use these experience points to purchase individual or packs of\nNFTrig cards, or they can be used to combine cards.\nREFERENCES\n[1] Rana M Amir Latif, Khalid Hussain, NZ Jhanjhi, Anand Nayyar, and Osama Rizwan. 2020. A remix IDE: smart\ncontract-based framework for the healthcare sector by using Blockchain technology. Multimedia Tools and Applications\n(2020), 1\u201324.\n[2] Mohsen Attaran and Angappa Gunasekaran. 2019. Blockchain for Gaming. In Applications of Blockchain Technology in\nBusiness. Springer, 85\u201388.\n[3] Rocsana Bucea-Manea-T, oni\u015f, Oliva Martins, Radu Bucea-Manea-T, oni\u015f, C\u0103t\u0103lin Gheorghit,\u0103, Valentin Kuleto, Milena P\nIli\u0107, and Violeta-Elena Simion. 2021. Blockchain Technology Enhances Sustainable Higher Education. Sustainability\n13, 22 (2021), 12347.\n[4] Juan Jos\u00e9 Bull\u00f3n, Ascensi\u00f3n Hern\u00e1ndez Encinas, M. Jes\u00fas Santos S\u00e1nchez, and V\u00edctor Gayoso Mart\u00ednez. 2018. Analysis\nof student feedback when using gamification tools in math subjects. In 2018 IEEE Global Engineering Education\nConference (EDUCON). 1818\u20131823. https://doi.org/10.1109/EDUCON.2018.8363455\n[5] Guang Chen, Bing Xu, Manli Lu, and Nian-Shing Chen. 2018. Exploring blockchain technology and its potential\napplications for education. Smart Learning Environments 5, 1 (2018), 1\u201310.\n[6] Luisanna Cocco, Andrea Pinna, and Michele Marchesi. 2017. Banking on blockchain: Costs savings thanks to the\nblockchain technology. Future internet 9, 3 (2017), 25.\n[7] Marco Conoscenti, Antonio Vetro, and Juan Carlos De Martin. 2016. Blockchain for the Internet of Things: A systematic\nliterature review. In 2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA). IEEE,\n1\u20136.\n[8] Oscar Delgado-Mohatar, Ruben Tolosana, Julian Fierrez, and Aythami Morales. 2020. Blockchain in the Internet of\nThings: Architectures and Implementation. In 2020 IEEE 44th Annual Computers, Software, and Applications Conference\n(COMPSAC). 1072\u20131077. https://doi.org/10.1109/COMPSAC48688.2020.0-131\n[9] A Elmessiry, M Elmessiry, and L Bridgesmith. 2021. NFT STUDENT TEACHER INCENTIVE SYSTEM (NFT-STIS). In\nProceedings of EDULEARN21 Conference, Vol. 5. 6th.\n[10] Usef Faghihi, Albert Brautigam, Kris Jorgenson, David Martin, Angela Brown, Elizabeth Measures, and Sioui Maldonado-\nBouchard. 2014. How Gamification Applies for Educational Purpose Specially with College Algebra. Procedia Computer\nScience 41 (2014), 182\u2013187.\nhttps://doi.org/10.1016/j.procs.2014.11.102 5th Annual International Conference on\nBiologically Inspired Cognitive Architectures, 2014 BICA.\n[11] Julian Alberto Garcia-Garcia, Nicol\u00e1s S\u00e1nchez-G\u00f3mez, David Lizcano, Mar\u00eda Jos\u00e9 Escalona, and Tom\u00e1s Wojdy\u0144ski.\n2020. Using blockchain to improve collaborative business process management: Systematic literature review. IEEE\nAccess 8 (2020), 142312\u2013142336.\n[12] Susan Gass, Koen Van Gorp, and Paula Winke. 2019. Using different carrots: How incentivization affects proficiency\ntesting outcomes. Foreign Language Annals 52, 2 (2019), 216\u2013236.\n[13] Ammar Yanuar Ghulam. 2021. Konseptual Desain Website Aplikasi Penyedia Jasa Kursus Mengemudi Mobil Di\nPurwokerto Menggunakan Framework Bootstrap 5. (2021).\n[14] Alexander Grech and Anthony F Camilleri. 2017. Blockchain in education. Luxembourg: Publications Office of the\nEuropean Union.\n[15] Marijn Haverbeke. 2018. Eloquent javascript: A modern introduction to programming. No Starch Press.\n[16] Marko H\u00f6lbl, Marko Kompara, Aida Kami\u0161ali\u0107, and Lili Nemec Zlatolas. 2018. A systematic review of the use of\nblockchain in healthcare. Symmetry 10, 10 (2018), 470.\nNFTrig: Using Blockchain Technologies for Math Education\n11\n[17] Carmen Holotescu et al. 2018. Understanding blockchain opportunities and challenges. In Conference proceedings of\u00bb\neLearning and Software for Education \u00ab(eLSE), Vol. 4. \u201d Carol I\u201d National Defence University Publishing House, 275\u2013283.\n[18] Tomislav Jagu\u0161t, Ivica Boti\u010dki, and Hyo-Jeong So. 2018. Examining competitive, collaborative and adaptive gamification\nin young learners\u2019 math learning. Computers Education 125 (2018), 444\u2013457. https://doi.org/10.1016/j.compedu.2018.\n06.022\n[19] Bruce Johnson. 2012. Professional visual studio 2012. John Wiley & Sons.\n[20] Arnav Kapoor, Dipanwita Guhathakurta, Mehul Mathur, Rupanshu Yadav, Manish Gupta, and Ponnurungam Ku-\nmaraguru. 2022. TweetBoost: Influence of Social Media on NFT Valuation. arXiv preprint arXiv:2201.08373 (2022).\n[21] Parth Khandelwal, Rahul Johari, Varnika Gaur, and Dharm Vashisth. 2021. BlockChain Technology based Smart\nContract Agreement on REMIX IDE. In 2021 8th International Conference on Signal Processing and Integrated Networks\n(SPIN). 938\u2013942. https://doi.org/10.1109/SPIN52536.2021.9565983\n[22] Kristin Kostick-Quenet, Kenneth D. Mandl, Timo Minssen, I. Glenn Cohen, Urs Gasser, Isaac Kohane, and Amy L.\nMcGuire. 2022. How NFTs could transform health information exchange. Science 375, 6580 (2022), 500\u2013502. https:\n//doi.org/10.1126/science.abm2004 arXiv:https://www.science.org/doi/pdf/10.1126/science.abm2004\n[23] J\u00f6rg Krause. 2020. Introduction to Bootstrap. In Introducing Bootstrap 4. Springer, 1\u201317.\n[24] Wei-Meng Lee. 2019. Using the metamask chrome extension. In Beginning Ethereum Smart Contracts Programming.\nSpringer, 93\u2013126.\n[25] Raoul LePage and Lynne Billard. 1992. Exploring the limits of bootstrap. Vol. 270. John Wiley & Sons.\n[26] Debajani Mohanty. 2018. Basic solidity programming. In Ethereum for Architects and Developers. Springer, 55\u2013103.\n[27] Haroon Shakirat Oluwatosin. 2014. Client-server model. IOSRJ Comput. Eng 16, 1 (2014), 2278\u20138727.\n[28] Deni Pramulia and Bayu Anggorojati. 2020. Implementation and evaluation of blockchain based e-voting system with\nEthereum and Metamask. In 2020 International Conference on Informatics, Multimedia, Cyber and Information System\n(ICIMCIS). 18\u201323. https://doi.org/10.1109/ICIMCIS51567.2020.9354310\n[29] R Raja and PC Nagasubramani. 2018. Impact of modern technology in education. Journal of Applied and Advanced\nResearch 3, 1 (2018), 33\u201335.\n[30] A Ravishankar Rao and Riddhi Dave. 2019. Developing hands-on laboratory exercises for teaching STEM students the\ninternet-of-things, cloud computing and blockchain applications. In 2019 IEEE Integrated STEM Education Conference\n(ISEC). IEEE, 191\u2013198.\n[31] Diane J Skiba et al. 2017. The potential of blockchain in education and health care. Nursing education perspectives 38, 4\n(2017), 220\u2013221.\n[32] Craig Standing. 2002. Methodologies for developing Web applications. Information and Software Technology 44, 3\n(2002), 151\u2013159. https://doi.org/10.1016/S0950-5849(02)00002-2\n[33] Qin Wang, Rujia Li, Qi Wang, and Shiping Chen. 2021. Non-fungible token (NFT): Overview, evaluation, opportunities\nand challenges. arXiv preprint arXiv:2105.07447 (2021).\n[34] Hafiza Yumna, Muhammad Murad Khan, Maria Ikram, and Sabahat Ilyas. 2019. Use of Blockchain in Education: A\nSystematic Literature Review. In Intelligent Information and Database Systems, Ngoc Thanh Nguyen, Ford Lumban\nGaol, Tzung-Pei Hong, and Bogdan Trawi\u0144ski (Eds.). Springer International Publishing, Cham, 191\u2013202.\n[35] Hafiza Yumna, Muhammad Murad Khan, Maria Ikram, and Sabahat Ilyas. 2019. Use of blockchain in education: a\nsystematic literature review. In Asian Conference on Intelligent Information and Database Systems. Springer, 191\u2013202.",
          "sections": [
            {
              "heading": "1\nINTRODUCTION\nThe purpose of this report is to describe the technical details involved in the development of the",
              "level": 1,
              "content": "NFTrig application. This includes both the front end website design, the back end smart contract,\nand NFT creation. It will mainly focus on the technical details of the project outlining software\nrequirements, design through programming languages, client and server side interactions, and\nvalidation testing. This allows the reader to undertake further development, fixes, or maintenance\nof the software, as this forms part of the documentation for the software.\nThe NFTrig project is based around the creation of a web-based game application that allows\ninteraction of NFTs (non-fungible token) with trigonometric function designs. NFts are digital\nassets, for example a picture, that has a unique identification and can generally be freely traded\nwith cryptocurrency [33]. Through this application, users are able to purchase digital artwork of\nmany different trigonometric functions and combine them using mathematical operations. Current\nsupported operations include multiplication and division o"
            },
            {
              "heading": "2\nMOTIVATION\nThe purpose of this application is as an educational tool for students who are attempting to",
              "level": 1,
              "content": "understand the ways that trigonometric functions interact with each other. As opposed to just\ngraphing these functions by hand, students will be able to generate new NFTs by combining\nwhatever trigonometric functions they already own. In fact, using technology is shown to influence\nand better educational processes by increasing interaction between those in the classroom [9].\nTechnology is becoming increasingly prevalent in every sphere of daily life, so the use of technology\nin a classroom setting is not only logical, but it increases the educational benefit of students [29].\nHowever, as the technology continues to evolve, \"the gap between traditional course material\ntaught to students in B.S./M.S. programs at universities and the cutting edge of technology used in\nindustry is widening at an unprecedented rate\" [30]. By creating this project, it will give students\nthe opportunity to gain experience with block chain, and hopefully be a starting place for narrowing\nthat ever growing gap."
            },
            {
              "heading": "3\nRELATED WORK\nBlock chain technology has enabled the formation of decentralized distributed records of digital",
              "level": 1,
              "content": "data which does not require any third party to moderate any transactions [34]. The decentralized\nnature of block chain also renders it easy for use in a ranging variety of applications in several fields\nsuch as healthcare [16], internet of things [7], gaming [2], banking [6], and education (explored in\ngreater detail in subsection 3.1). Non Fungible token (NFTs) are a relatively new phenomena within\nthe field of block chain based technologies, but its application in aforementioned fields are already\nbeing studied. Specifically within the healthcare context, NFT\u2019s are solving long term issues such as\nstoring patients\u2019 private data more safely as well as maintaining better records while giving better\nautonomy and privacy to both patients and healthcare providers [22]. The application of NFTs in\nNFTrig: Using Blockchain Technologies for Math Education\n3\neducation is still an understudied area. These next related work sections explore the broader use of\nblock chain based technologies for e"
            },
            {
              "heading": "4\nEXPERIMENTAL SETUP",
              "level": 1,
              "content": "4.1\nSoftware Development Requirements\nThe NFTrig application employs a variety of software development requirements that cover the\nrange of the project. From front end web development to back end smart contract creation and\nNFT storage, this section describes the requirements and software used to complete the project.\n4.1.1\nCompiling IDE. The smart contracts created for NFTrig are hosted on Remix. Remix is an\nan open source online compiler IDE that can be used to test and deploy smart contracts [1]. The\nNFTrig: Using Blockchain Technologies for Math Education\n5\nplatform can be accessed by any browser, and it allows the developer to write and deploy smart\ncontracts on an actual or test server simultaneously. The current deployment is on a test server. In\norder to test and debug the smart contract, Visual Studio Code is used. Visual Studio was found\nto be the best code editor because a developer can easily upload most file types, and edit them\n[19]. For NFTrig, it was used to develop fro"
            },
            {
              "heading": "5\nSOFTWARE DESIGN",
              "level": 1,
              "content": "This section covers all of the decisions necessary to understand the development of NFTrig, as well\nas the technical implementation of each technology used in the design process.\n6\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\n5.1\nSoftware Architecture\nThe architecture of this project follows the model-server design architecture [27]. Using this model,\nthe clients send transactions and requests to a proxy smart contract stored on the block chain\nwhich then makes the appropriate calls to the logic smart contract which is also stored on the block\nchain. This style of architecture is required for this project because the smart contracts must be\nstored on the server-side chain in order to be functional. The use of proxy contracts also allows our\nsmart contracts to be fully upgradeable with any future updates that may need to be implemented.\n5.2\nChoice of Programming Language\nThis section examines and explains the benefit of each chosen language"
            },
            {
              "heading": "6\nMETHODS",
              "level": 1,
              "content": "Most methods for completing this project have been thoroughly explained in the sections above.\nHowever, the final intended version of this project will be hosted in a different location than it\nresides currently. The initial portion of this project had the front end website hosted on a local\nAugustana College server and the back end smart contract hosted on the Polygon test net. This\nallowed initial testing and validation that the smart contract operated as expected, as well as give\ntime and opportunity to discover security vulnerabilities. The future of this project will be hosted\non a decentralized web application online so that users can access it and begin to interact with the\nsmart contract. Further, a redesign of the website user interface is likely. This will require transition\nfrom BootStrap5 to NextJS which allows cards to be generated, displayed, and interactable through\na version of JavaScript."
            },
            {
              "heading": "7\nRESULTS\nThis project successfully allowed the exploration and creation of applying NFT and block chain",
              "level": 1,
              "content": "technology to math education. Although preliminary in use and nature, this project allows for\ninitial project creation as a boiler plate project. The smart contract is currently deployed on the\n10\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\nPolygon testnet and can be interacted with using test Matic. Each web page has functionality to\ndisplay the user\u2019s owned NFTs as well as the NFTs they have put for sale on the marketplace. Using\nNextJS will also allow the Combination page to have functionality and smart contract use. It is also\nworth noting that the created web page is not required to interact with the NFTrig smart contracts."
            },
            {
              "heading": "8\nRECOMMENDATIONS FOR FUTURE WORK",
              "level": 1,
              "content": "The goal for this project was a working Beta demo that shows application functionality, and correct\nsmart contract execution. There are many other features planned for the continued work of this\nproject. The first, as earlier explained, is a game option which challenges the user with trigonometry\ntrivia and math problems. Answering these questions successfully will increase the experience\npoints of a user. The user can then use these experience points to purchase individual or packs of\nNFTrig cards, or they can be used to combine cards.\nREFERENCES\n[1] Rana M Amir Latif, Khalid Hussain, NZ Jhanjhi, Anand Nayyar, and Osama Rizwan. 2020. A remix IDE: smart\ncontract-based framework for the healthcare sector by using Blockchain technology. Multimedia Tools and Applications\n(2020), 1\u201324.\n[2] Mohsen Attaran and Angappa Gunasekaran. 2019. Blockchain for Gaming. In Applications of Blockchain Technology in\nBusiness. Springer, 85\u201388.\n[3] Rocsana Bucea-Manea-T, oni\u015f, Oliva Martins, Radu Bucea-Mane"
            },
            {
              "heading": "INTRODUCTION",
              "level": 1,
              "content": "The purpose of this report is to describe the technical details involved in the development of the\nNFTrig application. This includes both the front end website design, the back end smart contract,\nand NFT creation. It will mainly focus on the technical details of the project outlining software\nrequirements, design through programming languages, client and server side interactions, and\nvalidation testing. This allows the reader to undertake further development, fixes, or maintenance\nof the software, as this forms part of the documentation for the software.\nThe NFTrig project is based around the creation of a web-based game application that allows\ninteraction of NFTs (non-fungible token) with trigonometric function designs. NFts are digital\nassets, for example a picture, that has a unique identification and can generally be freely traded\nwith cryptocurrency [33]. Through this application, users are able to purchase digital artwork of\nmany different trigonometric functions and combine the"
            },
            {
              "heading": "MOTIVATION",
              "level": 1,
              "content": "The purpose of this application is as an educational tool for students who are attempting to\nunderstand the ways that trigonometric functions interact with each other. As opposed to just\ngraphing these functions by hand, students will be able to generate new NFTs by combining\nwhatever trigonometric functions they already own. In fact, using technology is shown to influence\nand better educational processes by increasing interaction between those in the classroom [9].\nTechnology is becoming increasingly prevalent in every sphere of daily life, so the use of technology\nin a classroom setting is not only logical, but it increases the educational benefit of students [29].\nHowever, as the technology continues to evolve, \"the gap between traditional course material\ntaught to students in B.S./M.S. programs at universities and the cutting edge of technology used in\nindustry is widening at an unprecedented rate\" [30]. By creating this project, it will give students\nthe opportunity to gain experi"
            },
            {
              "heading": "RELATED WORK",
              "level": 1,
              "content": "Block chain technology has enabled the formation of decentralized distributed records of digital\ndata which does not require any third party to moderate any transactions [34]. The decentralized\nnature of block chain also renders it easy for use in a ranging variety of applications in several fields\nsuch as healthcare [16], internet of things [7], gaming [2], banking [6], and education (explored in\ngreater detail in subsection 3.1). Non Fungible token (NFTs) are a relatively new phenomena within\nthe field of block chain based technologies, but its application in aforementioned fields are already\nbeing studied. Specifically within the healthcare context, NFT\u2019s are solving long term issues such as\nstoring patients\u2019 private data more safely as well as maintaining better records while giving better\nautonomy and privacy to both patients and healthcare providers [22]. The application of NFTs in\nNFTrig: Using Blockchain Technologies for Math Education\n3\neducation is still an understudied area."
            },
            {
              "heading": "EXPERIMENTAL SETUP",
              "level": 1,
              "content": "4.1\nSoftware Development Requirements\nThe NFTrig application employs a variety of software development requirements that cover the\nrange of the project. From front end web development to back end smart contract creation and\nNFT storage, this section describes the requirements and software used to complete the project.\n4.1.1\nCompiling IDE. The smart contracts created for NFTrig are hosted on Remix. Remix is an\nan open source online compiler IDE that can be used to test and deploy smart contracts [1]. The\nNFTrig: Using Blockchain Technologies for Math Education\n5\nplatform can be accessed by any browser, and it allows the developer to write and deploy smart\ncontracts on an actual or test server simultaneously. The current deployment is on a test server. In\norder to test and debug the smart contract, Visual Studio Code is used. Visual Studio was found\nto be the best code editor because a developer can easily upload most file types, and edit them\n[19]. For NFTrig, it was used to develop fro"
            },
            {
              "heading": "SOFTWARE DESIGN",
              "level": 1,
              "content": "This section covers all of the decisions necessary to understand the development of NFTrig, as well\nas the technical implementation of each technology used in the design process.\n6\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\n5.1\nSoftware Architecture\nThe architecture of this project follows the model-server design architecture [27]. Using this model,\nthe clients send transactions and requests to a proxy smart contract stored on the block chain\nwhich then makes the appropriate calls to the logic smart contract which is also stored on the block\nchain. This style of architecture is required for this project because the smart contracts must be\nstored on the server-side chain in order to be functional. The use of proxy contracts also allows our\nsmart contracts to be fully upgradeable with any future updates that may need to be implemented.\n5.2\nChoice of Programming Language\nThis section examines and explains the benefit of each chosen language"
            },
            {
              "heading": "METHODS",
              "level": 1,
              "content": "Most methods for completing this project have been thoroughly explained in the sections above.\nHowever, the final intended version of this project will be hosted in a different location than it\nresides currently. The initial portion of this project had the front end website hosted on a local\nAugustana College server and the back end smart contract hosted on the Polygon test net. This\nallowed initial testing and validation that the smart contract operated as expected, as well as give\ntime and opportunity to discover security vulnerabilities. The future of this project will be hosted\non a decentralized web application online so that users can access it and begin to interact with the\nsmart contract. Further, a redesign of the website user interface is likely. This will require transition\nfrom BootStrap5 to NextJS which allows cards to be generated, displayed, and interactable through\na version of JavaScript.\n7"
            },
            {
              "heading": "RESULTS",
              "level": 1,
              "content": "This project successfully allowed the exploration and creation of applying NFT and block chain\ntechnology to math education. Although preliminary in use and nature, this project allows for\ninitial project creation as a boiler plate project. The smart contract is currently deployed on the\n10\nJordan Thompson, Ryan Benac, Kidus Olana, Talha Hassan, Andrew Sward, and Tauheed Khan Mohd\nPolygon testnet and can be interacted with using test Matic. Each web page has functionality to\ndisplay the user\u2019s owned NFTs as well as the NFTs they have put for sale on the marketplace. Using\nNextJS will also allow the Combination page to have functionality and smart contract use. It is also\nworth noting that the created web page is not required to interact with the NFTrig smart contracts.\n8"
            }
          ],
          "references": [
            "[1] Rana M Amir Latif, Khalid Hussain, NZ Jhanjhi, Anand Nayyar, and Osama Rizwan. 2020. A remix IDE: smart\ncontract-based framework for the healthcare sector by using Blockchain technology. Multimedia Tools and Applications\n(2020), 1\u201324.",
            "Mohsen Attaran and Angappa Gunasekaran. 2019. Blockchain for Gaming. In Applications of Blockchain Technology in\nBusiness. Springer, 85\u201388.",
            "Rocsana Bucea-Manea-T, oni\u015f, Oliva Martins, Radu Bucea-Manea-T, oni\u015f, C\u0103t\u0103lin Gheorghit,\u0103, Valentin Kuleto, Milena P\nIli\u0107, and Violeta-Elena Simion. 2021. Blockchain Technology Enhances Sustainable Higher Education. Sustainability\n13, 22 (2021), 12347.",
            "Juan Jos\u00e9 Bull\u00f3n, Ascensi\u00f3n Hern\u00e1ndez Encinas, M. Jes\u00fas Santos S\u00e1nchez, and V\u00edctor Gayoso Mart\u00ednez. 2018. Analysis\nof student feedback when using gamification tools in math subjects. In 2018 IEEE Global Engineering Education\nConference (EDUCON). 1818\u20131823. https://doi.org/10.1109/EDUCON.2018.8363455",
            "Guang Chen, Bing Xu, Manli Lu, and Nian-Shing Chen. 2018. Exploring blockchain technology and its potential\napplications for education. Smart Learning Environments 5, 1 (2018), 1\u201310.",
            "Luisanna Cocco, Andrea Pinna, and Michele Marchesi. 2017. Banking on blockchain: Costs savings thanks to the\nblockchain technology. Future internet 9, 3 (2017), 25.",
            "Marco Conoscenti, Antonio Vetro, and Juan Carlos De Martin. 2016. Blockchain for the Internet of Things: A systematic\nliterature review. In 2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA). IEEE,\n1\u20136.",
            "Oscar Delgado-Mohatar, Ruben Tolosana, Julian Fierrez, and Aythami Morales. 2020. Blockchain in the Internet of\nThings: Architectures and Implementation. In 2020 IEEE 44th Annual Computers, Software, and Applications Conference\n(COMPSAC). 1072\u20131077. https://doi.org/10.1109/COMPSAC48688.2020.0-131",
            "A Elmessiry, M Elmessiry, and L Bridgesmith. 2021. NFT STUDENT TEACHER INCENTIVE SYSTEM (NFT-STIS). In\nProceedings of EDULEARN21 Conference, Vol. 5. 6th.",
            "Usef Faghihi, Albert Brautigam, Kris Jorgenson, David Martin, Angela Brown, Elizabeth Measures, and Sioui Maldonado-\nBouchard. 2014. How Gamification Applies for Educational Purpose Specially with College Algebra. Procedia Computer\nScience 41 (2014), 182\u2013187.\nhttps://doi.org/10.1016/j.procs.2014.11.102 5th Annual International Conference on\nBiologically Inspired Cognitive Architectures, 2014 BICA.",
            "Julian Alberto Garcia-Garcia, Nicol\u00e1s S\u00e1nchez-G\u00f3mez, David Lizcano, Mar\u00eda Jos\u00e9 Escalona, and Tom\u00e1s Wojdy\u0144ski.",
            "Using blockchain to improve collaborative business process management: Systematic literature review. IEEE\nAccess 8 (2020), 142312\u2013142336.",
            "Susan Gass, Koen Van Gorp, and Paula Winke. 2019. Using different carrots: How incentivization affects proficiency\ntesting outcomes. Foreign Language Annals 52, 2 (2019), 216\u2013236.",
            "Ammar Yanuar Ghulam. 2021. Konseptual Desain Website Aplikasi Penyedia Jasa Kursus Mengemudi Mobil Di\nPurwokerto Menggunakan Framework Bootstrap 5. (2021).",
            "Alexander Grech and Anthony F Camilleri. 2017. Blockchain in education. Luxembourg: Publications Office of the\nEuropean Union.",
            "Marijn Haverbeke. 2018. Eloquent javascript: A modern introduction to programming. No Starch Press.",
            "Marko H\u00f6lbl, Marko Kompara, Aida Kami\u0161ali\u0107, and Lili Nemec Zlatolas. 2018. A systematic review of the use of\nblockchain in healthcare. Symmetry 10, 10 (2018), 470.\nNFTrig: Using Blockchain Technologies for Math Education\n11",
            "Carmen Holotescu et al. 2018. Understanding blockchain opportunities and challenges. In Conference proceedings of\u00bb\neLearning and Software for Education \u00ab(eLSE), Vol. 4. \u201d Carol I\u201d National Defence University Publishing House, 275\u2013283.",
            "Tomislav Jagu\u0161t, Ivica Boti\u010dki, and Hyo-Jeong So. 2018. Examining competitive, collaborative and adaptive gamification\nin young learners\u2019 math learning. Computers Education 125 (2018), 444\u2013457. https://doi.org/10.1016/j.compedu.2018.",
            "Bruce Johnson. 2012. Professional visual studio 2012. John Wiley & Sons.",
            "Arnav Kapoor, Dipanwita Guhathakurta, Mehul Mathur, Rupanshu Yadav, Manish Gupta, and Ponnurungam Ku-\nmaraguru. 2022. TweetBoost: Influence of Social Media on NFT Valuation. arXiv preprint arXiv:2201.08373 (2022).",
            "Parth Khandelwal, Rahul Johari, Varnika Gaur, and Dharm Vashisth. 2021. BlockChain Technology based Smart\nContract Agreement on REMIX IDE. In 2021 8th International Conference on Signal Processing and Integrated Networks\n(SPIN). 938\u2013942. https://doi.org/10.1109/SPIN52536.2021.9565983",
            "Kristin Kostick-Quenet, Kenneth D. Mandl, Timo Minssen, I. Glenn Cohen, Urs Gasser, Isaac Kohane, and Amy L.\nMcGuire. 2022. How NFTs could transform health information exchange. Science 375, 6580 (2022), 500\u2013502. https:\n//doi.org/10.1126/science.abm2004 arXiv:https://www.science.org/doi/pdf/10.1126/science.abm2004",
            "J\u00f6rg Krause. 2020. Introduction to Bootstrap. In Introducing Bootstrap 4. Springer, 1\u201317.",
            "Wei-Meng Lee. 2019. Using the metamask chrome extension. In Beginning Ethereum Smart Contracts Programming.\nSpringer, 93\u2013126.",
            "Raoul LePage and Lynne Billard. 1992. Exploring the limits of bootstrap. Vol. 270. John Wiley & Sons.",
            "Debajani Mohanty. 2018. Basic solidity programming. In Ethereum for Architects and Developers. Springer, 55\u2013103.",
            "Haroon Shakirat Oluwatosin. 2014. Client-server model. IOSRJ Comput. Eng 16, 1 (2014), 2278\u20138727.",
            "Deni Pramulia and Bayu Anggorojati. 2020. Implementation and evaluation of blockchain based e-voting system with\nEthereum and Metamask. In 2020 International Conference on Informatics, Multimedia, Cyber and Information System\n(ICIMCIS). 18\u201323. https://doi.org/10.1109/ICIMCIS51567.2020.9354310",
            "R Raja and PC Nagasubramani. 2018. Impact of modern technology in education. Journal of Applied and Advanced\nResearch 3, 1 (2018), 33\u201335.",
            "A Ravishankar Rao and Riddhi Dave. 2019. Developing hands-on laboratory exercises for teaching STEM students the\ninternet-of-things, cloud computing and blockchain applications. In 2019 IEEE Integrated STEM Education Conference\n(ISEC). IEEE, 191\u2013198.",
            "Diane J Skiba et al. 2017. The potential of blockchain in education and health care. Nursing education perspectives 38, 4\n(2017), 220\u2013221.",
            "Craig Standing. 2002. Methodologies for developing Web applications. Information and Software Technology 44, 3\n(2002), 151\u2013159. https://doi.org/10.1016/S0950-5849(02)00002-2",
            "Qin Wang, Rujia Li, Qi Wang, and Shiping Chen. 2021. Non-fungible token (NFT): Overview, evaluation, opportunities\nand challenges. arXiv preprint arXiv:2105.07447 (2021).",
            "Hafiza Yumna, Muhammad Murad Khan, Maria Ikram, and Sabahat Ilyas. 2019. Use of Blockchain in Education: A\nSystematic Literature Review. In Intelligent Information and Database Systems, Ngoc Thanh Nguyen, Ford Lumban\nGaol, Tzung-Pei Hong, and Bogdan Trawi\u0144ski (Eds.). Springer International Publishing, Cham, 191\u2013202.",
            "Hafiza Yumna, Muhammad Murad Khan, Maria Ikram, and Sabahat Ilyas. 2019. Use of blockchain in education: a\nsystematic literature review. In Asian Conference on Intelligent Information and Database Systems. Springer, 191\u2013202."
          ],
          "metadata": {
            "source": "data/sample_papers/2301.00001.pdf",
            "page_count": 11,
            "word_count": 5996,
            "extracted_at": "2025-11-25T22:24:07.351469"
          }
        },
        "critic_analysis": {
          "overall_score": 7.025,
          "strengths": [
            "Strong clarity: well-executed and clearly presented",
            "Comprehensive literature review with extensive citations",
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "Minor presentation improvements needed"
          ],
          "detailed_scores": {
            "novelty": 7.0,
            "methodology": 7.3,
            "clarity": 7.7,
            "reproducibility": 6.1
          },
          "recommendations": [
            "Consider adding more ablation studies",
            "Expand discussion of limitations"
          ],
          "methodology_analysis": {
            "has_methodology_section": true,
            "methodology_length": 1838,
            "experimental_design": "present"
          },
          "clarity_metrics": {
            "abstract_length": 5996,
            "section_count": 15,
            "reference_count": 36,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 36,
          "key_citations": [
            {
              "text": "[1] Rana M Amir Latif, Khalid Hussain, NZ Jhanjhi, Anand Nayyar, and Osama Rizwan. 2020. A remix IDE: smart\ncontract-based framework for the healthcare sector by using Blockchain technology. Multimedi",
              "year": 2020,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Mohsen Attaran and Angappa Gunasekaran. 2019. Blockchain for Gaming. In Applications of Blockchain Technology in\nBusiness. Springer, 85\u201388.",
              "year": 2019,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Rocsana Bucea-Manea-T, oni\u015f, Oliva Martins, Radu Bucea-Manea-T, oni\u015f, C\u0103t\u0103lin Gheorghit,\u0103, Valentin Kuleto, Milena P\nIli\u0107, and Violeta-Elena Simion. 2021. Blockchain Technology Enhances Sustainable Hi",
              "year": 2021,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Juan Jos\u00e9 Bull\u00f3n, Ascensi\u00f3n Hern\u00e1ndez Encinas, M. Jes\u00fas Santos S\u00e1nchez, and V\u00edctor Gayoso Mart\u00ednez. 2018. Analysis\nof student feedback when using gamification tools in math subjects. In 2018 IEEE Glob",
              "year": 2018,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Guang Chen, Bing Xu, Manli Lu, and Nian-Shing Chen. 2018. Exploring blockchain technology and its potential\napplications for education. Smart Learning Environments 5, 1 (2018), 1\u201310.",
              "year": 2018,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Luisanna Cocco, Andrea Pinna, and Michele Marchesi. 2017. Banking on blockchain: Costs savings thanks to the\nblockchain technology. Future internet 9, 3 (2017), 25.",
              "year": 2017,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Marco Conoscenti, Antonio Vetro, and Juan Carlos De Martin. 2016. Blockchain for the Internet of Things: A systematic\nliterature review. In 2016 IEEE/ACS 13th International Conference of Computer Syst",
              "year": 2016,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Oscar Delgado-Mohatar, Ruben Tolosana, Julian Fierrez, and Aythami Morales. 2020. Blockchain in the Internet of\nThings: Architectures and Implementation. In 2020 IEEE 44th Annual Computers, Software, ",
              "year": 2020,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "A Elmessiry, M Elmessiry, and L Bridgesmith. 2021. NFT STUDENT TEACHER INCENTIVE SYSTEM (NFT-STIS). In\nProceedings of EDULEARN21 Conference, Vol. 5. 6th.",
              "year": 2021,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "Usef Faghihi, Albert Brautigam, Kris Jorgenson, David Martin, Angela Brown, Elizabeth Measures, and Sioui Maldonado-\nBouchard. 2014. How Gamification Applies for Educational Purpose Specially with Col",
              "year": 2014,
              "relevance": "high",
              "type": "unknown"
            }
          ],
          "related_papers": [
            {
              "arxiv_id": "2301.00001v1",
              "title": "NFTrig",
              "authors": [
                "Jordan Thompson",
                "Ryan Benac",
                "Kidus Olana"
              ],
              "abstract": "NFTrig is a web-based application created for use as an educational tool to teach trigonometry and block chain technology. Creation of the application includes front and back end development as well as integration with other outside sources including MetaMask and OpenSea. The primary development lan",
              "published": "2022-12-21T18:07:06+00:00",
              "similarity_score": 0.17647058823529413,
              "url": "http://arxiv.org/abs/2301.00001v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1009.5557v1",
              "title": "A Mobile Application for Smart House Remote Control System",
              "authors": [
                "Amir Rajabzadeh",
                "Ali Reza Manashty",
                "Zahra Forootan Jahromi"
              ],
              "abstract": "At the start of the second decade of 21th century, the time has come to make the Smart Houses a reality for regular use. The different parts of a Smart House are researched but there are still distances from an applicable system, using the modern technology. In this paper we present an overview of t",
              "published": "2010-09-27T18:13:37+00:00",
              "similarity_score": 0.14444444444444443,
              "url": "http://arxiv.org/abs/1009.5557v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1807.03111v1",
              "title": "A Framework for Detecting and Translating User Behavior from Smart Meter Data",
              "authors": [
                "Egon Kidmose",
                "Emad Ebeid",
                "Rune Hylsberg Jacobsen"
              ],
              "abstract": "The European adoption of smart electricity meters triggers the developments of new value-added service for smart energy and optimal consumption. Recently, several algorithms and tools have been built to analyze smart meter's data. This paper introduces an open framework and prototypes for detecting ",
              "published": "2018-06-13T10:03:44+00:00",
              "similarity_score": 0.1361111111111111,
              "url": "http://arxiv.org/abs/1807.03111v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2209.00778v2",
              "title": "Detection of False Data Injection Attacks in Smart Grid: A Secure Federated Deep Learning Approach",
              "authors": [
                "Yang Li",
                "Xinhao Wei",
                "Yuanzheng Li"
              ],
              "abstract": "As an important cyber-physical system (CPS), smart grid is highly vulnerable to cyber attacks. Amongst various types of attacks, false data injection attack (FDIA) proves to be one of the top-priority cyber-related issues and has received increasing attention in recent years. However, so far little ",
              "published": "2022-09-02T01:44:24+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2209.00778v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2111.06116v1",
              "title": "Implementation of Ethically Aligned Design with Ethical User stories in SMART terminal Digitalization project: Use case Passenger Flow",
              "authors": [
                "Erika Halme",
                "Mamia Agbese",
                "Hanna-Kaisa Alanen"
              ],
              "abstract": "Digitalization and Smart systems are part of our everyday lives today. So far the development has been rapid and all the implications that comes after the deployment has not been able to foresee or even assess during the development, especially when ethics or trustworthiness is concerned. Artificial",
              "published": "2021-11-11T09:27:38+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2111.06116v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1807.05347v2",
              "title": "Smart Grid Monitoring Using Power Line Modems: Anomaly Detection and Localization",
              "authors": [
                "Federico Passerini",
                "Andrea M. Tonello"
              ],
              "abstract": "The main subject of this paper is the sensing of network anomalies that span from harmless impedance changes at some network termination to more or less pronounced electrical faults, considering also cable degradation over time. In this paper, we present how to harvest information about such anomali",
              "published": "2018-07-14T07:27:47+00:00",
              "similarity_score": 0.07990430622009569,
              "url": "http://arxiv.org/abs/1807.05347v2",
              "reason": "Shared focus on: using"
            },
            {
              "arxiv_id": "1501.07329v4",
              "title": "A Big Data Architecture Design for Smart Grids Based on Random Matrix Theory",
              "authors": [
                "X. He",
                "Q. Ai",
                "C. Qiu"
              ],
              "abstract": "Model-based analysis tools, built on assumptions and simplifications, are difficult to handle smart grids with data characterized by 4Vs data. This paper, using random matrix theory (RMT), motivates data-driven tools to perceive the complex grids in highdimension; meanwhile, an architecture with det",
              "published": "2015-01-29T01:53:34+00:00",
              "similarity_score": 0.0757085020242915,
              "url": "http://arxiv.org/abs/1501.07329v4",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1809.00291v1",
              "title": "A Critical Look at Smart Wheelchairs",
              "authors": [
                "Benjamin Narin",
                "Makenzie Brian",
                "William D. Smart"
              ],
              "abstract": "Research into smart wheelchairs has been conducted for decades, but we have yet to see the widespread use of this technology among full-time wheelchair users. We argue that the main reason for this is that there is a mismatch between research and the actualities of using a powered mobility device in",
              "published": "2018-09-02T04:54:23+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/1809.00291v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2305.17247v1",
              "title": "The Araucaria Project: Improving the cosmic distance scale",
              "authors": [
                "The Araucaria Project",
                ":",
                "G. Pietrzy\u0144ski"
              ],
              "abstract": "The book consists of a number of short articles that present achievements of the Araucaria members, collaborators, and friends, in various aspects of distance determinations and related topics. It celebrates the 20-year anniversary of the Araucaria Project, acknowledges the people who worked for its",
              "published": "2023-05-26T20:28:39+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2305.17247v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "9903035v1",
              "title": "The ATLAS Pixel Project",
              "authors": [
                "John Richardson",
                "representing the ATLAS Pixel Project"
              ],
              "abstract": "The ATLAS experiment, at the Large Hadron Collider, will incorporate discrete, high-resolution tracking sub-systems in the form of segmented silicon detectors with 40MHz radiation-hard readout electronics. In the region closest to the pp interaction point, the thin silicon tiles will be segmented in",
              "published": "1999-03-16T21:10:40+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/hep-ex/9903035v1",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "The paper cites 36 references, indicating a solid grounding in related work. 12 references are from recent work (2020+), with opportunity to include more recent work.",
          "citation_network": {
            "total_citations": 36,
            "related_papers_found": 10,
            "network_density": 0.2777777777777778,
            "avg_similarity": 0.09927559110995643
          },
          "topics": [
            "Machine Learning",
            "Computer Vision",
            "Natural Language"
          ]
        },
        "final_review": {
          "executive_summary": "**NFTrig: Using Blockchain Technologies for Math Education**\n\n**Quick Assessment**: Overall Score 7.0/10\n\n**Main Contribution**: NFTrig: Using Blockchain Technologies for Math Education\nJORDAN THOMPSON, Augustana College, USA\nRYAN BENAC, Augustana College, USA\nKIDUS OLANA, Augustana College, USA\nTALHA HASSAN, Augustana College, USA\nANDREW SWARD, Augustana College, USA\nTAUHEED KHAN MOHD, Augustana College, USA\nNFTrig is a web-based application created for use as an educational tool to teach trigonometry and block\nchain technology.\n\n**Verdict**: This paper presents a strong contribution to the field.",
          "detailed_review": "# Detailed Review: NFTrig: Using Blockchain Technologies for Math Education\n\n## Overview\nNFTrig: Using Blockchain Technologies for Math Education\nJORDAN THOMPSON, Augustana College, USA\nRYAN BENAC, Augustana College, USA\nKIDUS OLANA, Augustana College, USA\nTALHA HASSAN, Augustana College, USA\nANDREW SWARD, Augustana College, USA\nTAUHEED KHAN MOHD, Augustana College, USA\nNFTrig is a web-based application created for use as an educational tool to teach trigonometry and block\nchain technology. Creation of the application includes front and back end development as well as integration\nwi...\n\n## Assessment Scores\n- **Novelty**: 7.0/10\n- **Methodology**: 7.3/10\n- **Clarity**: 7.7/10\n- **Reproducibility**: 6.1/10\n\n## Strengths\n1. Strong clarity: well-executed and clearly presented\n2. Comprehensive literature review with extensive citations\n3. Well-structured paper with detailed sections\n4. Detailed abstract providing good overview\n\n## Weaknesses\n1. Minor presentation improvements needed\n\n## Recommendations for Improvement\n1. Consider adding more ablation studies\n2. Expand discussion of limitations\n\n## Related Work\n- NFTrig (Similarity: 0.18)\n- A Mobile Application for Smart House Remote Control System (Similarity: 0.14)\n- A Framework for Detecting and Translating User Behavior from Smart Meter Data (Similarity: 0.14)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. NFTrig is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work is important because it shows a new way to tackle this problem that works really well!\n\n**The bottom line:**\nThe main good thing: Strong clarity: well-executed and clearly presented\nSomething to improve: Minor presentation improvements needed",
          "key_takeaways": [
            "Main focus: NFTrig: Using Blockchain Technologies for Math Education",
            "Key strength: Strong clarity: well-executed and clearly presented",
            "Strongest aspect: clarity (7.7/10)",
            "Area for improvement: Minor presentation improvements needed",
            "Overall quality: 7.0/10 - Good"
          ],
          "recommendation": "Weak Accept - Good work but needs improvements",
          "confidence": 0.94,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                7.0,
                7.3,
                7.7,
                6.1
              ]
            },
            "summary_card": {
              "title": "NFTrig: Using Blockchain Technologies for Math Education",
              "overall_score": 7.025,
              "recommendation": "Weak Accept - Good work but needs improvements"
            },
            "metrics": {
              "sections": 15,
              "references": 36,
              "word_count": 5996
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 4.648732,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:24:06.524244",
          "end_time": "2025-11-25T22:24:11.172976"
        }
      },
      "constraint_violations": [],
      "expectations_met": true,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 7.025
      }
    },
    {
      "test_id": "test_4",
      "name": "Citation-Heavy Paper",
      "timestamp": "2025-11-25T22:24:11.174277",
      "passed": false,
      "processing_time": 4.713354,
      "review_result": {
        "session_id": "6fcab3b1-bb71-48f1-b34a-e63c9e824526",
        "paper_content": {
          "paper_id": "paper-jacobdevlinmingweich-bf8b6496",
          "title": "{jacobdevlin,mingweichang,kentonl,kristout}@google.com",
          "authors": [
            "Language Understanding",
            "Jacob Devlin",
            "Kenton Lee",
            "Kristina Toutanova",
            "Bidirectional Encoder Representations from"
          ],
          "abstract": "We introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1",
          "sections": [
            {
              "heading": "1\nIntroduction",
              "level": 1,
              "content": "Language model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning app"
            },
            {
              "heading": "2\nRelated Work",
              "level": 1,
              "content": "There is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n2.1\nUnsupervised Feature-based Approaches\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or par"
            },
            {
              "heading": "3\nBERT",
              "level": 1,
              "content": "We introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning.\nDur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks.\nFor \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\nModel Architecture\nBERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and releas"
            },
            {
              "heading": "4\nExperiments",
              "level": 1,
              "content": "In this section, we present BERT \ufb01ne-tuning re-\nsults on 11 NLP tasks.\n4.1\nGLUE\nThe General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nlection of diverse natural language understanding\ntasks. Detailed descriptions of GLUE datasets are\nincluded in Appendix B.1.\nTo \ufb01ne-tune on GLUE, we represent the input\nsequence (for single sentence or sentence pairs)\nas described in Section 3, and use the \ufb01nal hid-\nden vector C \u2208RH corresponding to the \ufb01rst\ninput token ([CLS]) as the aggregate representa-\ntion. The only new parameters introduced during\n\ufb01ne-tuning are classi\ufb01cation layer weights W \u2208\nRK\u00d7H, where K is the number of labels. We com-\npute a standard classi\ufb01cation loss with C and W,\ni.e., log(softmax(CW T )).\n7For example, the BERT SQuAD model can be trained in\naround 30 minutes on a single Cloud TPU to achieve a Dev\nF1 score of 91.0%.\n8See (10) in https://gluebenchmark.com/faq.\nSystem\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n392k\n363k\n108"
            },
            {
              "heading": "5\nAblation Studies",
              "level": 1,
              "content": "In this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional\nDev Set\nTasks\nMNLI-m QNLI MRPC SST-2 SQuAD\n(Acc)\n(Acc)\n(Acc)\n(Acc)\n(F1)\nBERTBASE\n84.4\n88.4\n86.7\n92.7\n88.5\nNo NSP\n83.9\n84.9\n86.5\n92.6\n87.9\nLTR & No NSP\n82.1\n84.3\n77.5\n92.1\n77.8\n+ BiLSTM\n82.1\n84.1\n75.7\n91.6\n84.9\nTable 5: Ablation over the pre-training tasks using the\nBERTBASE architecture. \u201cNo NSP\u201d is trained without\nthe next sentence prediction task. \u201cLTR & No NSP\u201d is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran-\ndomly initialized BiLSTM on top of the \u201cLTR + No\nNSP\u201d model during \ufb01ne-tuning.\nablation studies can be found in Appendix C.\n5.1\nEffect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, \ufb01ne-tuning scheme, and hyperpa-\nrameters as BERTBASE:\nNo NSP:"
            },
            {
              "heading": "6\nConclusion\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated",
              "level": 1,
              "content": "that rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\nReferences\nAlan Akbik, Duncan Blythe, and Roland Vollgraf."
            },
            {
              "heading": "2018. Contextual string embeddings for sequence",
              "level": 2,
              "content": "labeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638\u20131649.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018.\nCharacter-level lan-\nguage modeling with deeper self-attention.\narXiv\npreprint arXiv:1808.04444.\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\nLuisa Bentivogli,\nBernardo Magnini,\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\n"
            },
            {
              "heading": "BERT\nBERT",
              "level": 1,
              "content": "E[CLS]\nE1\n E[SEP]\n..."
            },
            {
              "heading": "EN",
              "level": 1,
              "content": "E1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n..."
            },
            {
              "heading": "TN",
              "level": 1,
              "content": "T1\u2019\n...\nTM\u2019\n[CLS]\nTok 1\n [SEP]\n...\nTok N\nTok 1\n...\nTokM\nQuestion\nParagraph\nStart/End Span"
            },
            {
              "heading": "BERT",
              "level": 1,
              "content": "E[CLS]\nE1\n E[SEP]\n..."
            },
            {
              "heading": "EN",
              "level": 1,
              "content": "E1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n..."
            },
            {
              "heading": "TN",
              "level": 1,
              "content": "T1\u2019\n...\nTM\u2019\n[CLS]\nTok 1\n [SEP]\n...\nTok N\nTok 1\n...\nTokM\nMasked Sentence A\nMasked Sentence B\nPre-training\nFine-Tuning"
            },
            {
              "heading": "NSP",
              "level": 1,
              "content": "Mask LM\nMask LM\nUnlabeled Sentence A and B Pair \nSQuAD\nQuestion Answer Pair"
            },
            {
              "heading": "NER\nMNLI",
              "level": 1,
              "content": "Figure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n2.3\nTransfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune"
            }
          ],
          "references": [
            "Alan Akbik, Duncan Blythe, and Roland Vollgraf.",
            "Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638\u20131649.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018.\nCharacter-level lan-\nguage modeling with deeper self-attention.\narXiv\npreprint arXiv:1808.04444.\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\nLuisa Bentivogli,\nBernardo Magnini,\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.",
            "Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics, 18(4):467\u2013479.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018.\nSimple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018.\nSemi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013",
            "Ronan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning.\nIn Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.",
            "Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\nDan Hendrycks and Kevin Gimpel. 2016.\nBridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. 2018.\nReinforced\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294\u20133302.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\ntations.\nIn International Conference on Learning\nRepresentations.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\nOren Melamud, Jacob Goldberger, and Ido Dagan.",
            "context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model.\nIn\nD. Koller, D. Schuurmans, Y. Bengio, and L. Bot-\ntou, editors, Advances in Neural Information Pro-\ncessing Systems 21, pages 1081\u20131088. Curran As-\nsociates, Inc.\nAnkur P Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In EMNLP.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532\u2013",
            "Matthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In NAACL.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b.\nDissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1499\u20131509.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383\u20132392.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\n\ufb02ow for machine comprehension. In ICLR.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank.\nIn Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631\u20131642.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.",
            "U-net:\nMachine reading comprehension\nwith unanswerable questions.\narXiv preprint\narXiv:1810.06638.\nWilson L Taylor. 1953.\nCloze procedure:\nA new\ntool for measuring readability. Journalism Bulletin,\n30(4):415\u2013433.\nErik F Tjong Kim Sang and Fien De Meulder.",
            "Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008.\nExtracting and\ncomposing robust features with denoising autoen-\ncoders.\nIn Proceedings of the 25th international\nconference on Machine learning, pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353\u2013355.\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018.\nNeural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn\nNAACL.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe,\nMohammad Norouzi,\nWolfgang Macherey,\nMaxim Krikun,\nYuan Cao,\nQin Gao,\nKlaus\nMacherey, et al. 2016.\nGoogle\u2019s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation.\narXiv preprint\narXiv:1609.08144.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320\u20133328.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018.\nQANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books.\nIn Proceedings of the IEEE\ninternational conference on computer vision, pages\n19\u201327.\nAppendix for \u201cBERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding\u201d\nWe organize the appendix into three sections:\n\u2022 Additional implementation details for BERT\nare presented in Appendix A;\n\u2022 Additional details for our experiments are\npresented in Appendix B; and\n\u2022 Additional ablation studies are presented in\nAppendix C.\nWe present additional ablation studies for\nBERT including:\n\u2013 Effect of Number of Training Steps; and\n\u2013 Ablation for Different Masking Proce-\ndures.\nA\nAdditional Details for BERT\nA.1\nIllustration of the Pre-training Tasks\nWe provide examples of the pre-training tasks in\nthe following.\nMasked LM and the Masking Procedure\nAs-\nsuming the unlabeled sentence is\nmy dog is\nhairy, and during the random masking procedure\nwe chose the 4-th token (which corresponding to\nhairy), our masking procedure can be further il-\nlustrated by\n\u2022 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy \u2192\nmy dog is [MASK]\n\u2022 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy \u2192my\ndog is apple\n\u2022 10% of the time:\nKeep the word un-\nchanged, e.g., my dog is hairy \u2192my dog\nis hairy. The purpose of this is to bias the\nrepresentation towards the actual observed\nword.\nThe advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\nery input token.\nAdditionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model\u2019s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model\nBERT (Ours)\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n...\n...\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n...\n...\nOpenAI GPT\nLstm\nELMo\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\n T1\nT2\n TN\n...\n...\n...\n...\n...\n E1\nE2\n EN\n...\n T1\nT2\nTN\n...\n E1\nE2\n EN\n...\n T1\nT2\n TN\n...\n E1\nE2\n EN\n...\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are \ufb01ne-tuning approaches, while ELMo is a feature-based approach.\nto converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost.\nNext Sentence Prediction\nThe next sentence\nprediction task can be illustrated in the following\nexamples.\nInput = [CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel = IsNext\nInput = [CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel = NotNext\nA.2\nPre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the B embed-\nding. 50% of the time B is the actual next sentence\nthat follows A and 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is \u2264512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40\nepochs over the 3.3 billion word corpus.\nWe\nuse Adam with learning rate of 1e-4, \u03b21 = 0.9,\n\u03b22 = 0.999, L2 weight decay of 0.01, learning\nrate warmup over the \ufb01rst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu, following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERTBASE was performed on 4\nCloud TPUs in Pod con\ufb01guration (16 TPU chips\ntotal).13 Training of BERTLARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings.\nA.3\nFine-tuning Procedure\nFor \ufb01ne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speci\ufb01c, but we found the following range\nof possible values to work well across all tasks:\n\u2022 Batch size: 16, 32\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html\n\u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5\n\u2022 Number of epochs: 2, 3, 4\nWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set.\nA.4\nComparison of BERT, ELMo ,and\nOpenAI GPT\nHere we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are \ufb01ne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n\u2022 GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).\n\u2022 GPT uses a sentence separator ([SEP]) and\nclassi\ufb01er token ([CLS]) which are only in-\ntroduced at \ufb01ne-tuning time; BERT learns\n[SEP], [CLS] and sentence A/B embed-\ndings during pre-training.\n\u2022 GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words.\n\u2022 GPT used the same learning rate of 5e-5 for\nall \ufb01ne-tuning experiments; BERT chooses a\ntask-speci\ufb01c \ufb01ne-tuning learning rate which\nperforms the best on the development set.\nTo isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable.\nA.5\nIllustrations of Fine-tuning on Different\nTasks\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\nB\nDetailed Experimental Setup\nB.1\nDetailed Descriptions for the GLUE\nBenchmark Experiments.\nOur\nGLUE\nresults\nin\nTable1\nare\nobtained\nfrom\nhttps://gluebenchmark.com/\nleaderboard\nand\nhttps://blog.\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\nMNLI\nMulti-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\nQQP\nQuora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\nQNLI\nQuestion Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\nBERT\nE[CLS]\nE1\n E[SEP]\n...\nEN\nE1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n...\nTN\nT1\u2019\n...\nTM\u2019\n[CLS]\nTok \n1\n [SEP]\n...\nTok \nN\nTok \n1\n...\nTok\nM\nQuestion\nParagraph\nBERT\nE[CLS]\nE1\n E2\n EN\nC\nT1\n T2\n TN\nSingle Sentence \n...\n...\nBERT\nTok 1\n Tok 2\n Tok N\n...\n[CLS]\nE[CLS]\nE1\n E2\n EN\nC\nT1\n T2\n TN\nSingle Sentence \nB-PER\nO\nO\n...\n...\nE[CLS]\nE1\n E[SEP]\nClass \nLabel\n...\nEN\nE1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n...\nTN\nT1\u2019\n...\nTM\u2019\nStart/End Span\nClass \nLabel\nBERT\nTok 1\n Tok 2\n Tok N\n...\n[CLS]\nTok 1\n[CLS]\n[CLS]\nTok \n1\n [SEP]\n...\nTok \nN\nTok \n1\n...\nTok\nM\nSentence 1\n...\nSentence 2\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\nSST-2\nThe Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\nCoLA\nThe Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\nSTS-B\nThe Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\nMRPC\nMicrosoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\nRTE\nRecognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\nWNLI\nWinograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\njority class.\nC\nAdditional Ablation Studies\nC.1\nEffect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:",
            "Question:\nDoes BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost",
            "0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.",
            "Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2\nAblation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200\n400\n600\n800\n1,000\n76\n78\n80\n82\n84\nPre-training Steps (Thousands)\nMNLI Dev Accuracy\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.\nNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates\nDev Set Results\nMASK SAME\nRND\nMNLI\nNER\nFine-tune Fine-tune Feature-based\n80%\n10%\n10%",
            "6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well."
          ],
          "metadata": {
            "source": "data/sample_papers/citation_heavy.pdf",
            "page_count": 16,
            "word_count": 10152,
            "extracted_at": "2025-11-25T22:24:11.264200"
          }
        },
        "critic_analysis": {
          "overall_score": 7.075,
          "strengths": [
            "Strong methodology: well-executed and clearly presented",
            "Strong clarity: well-executed and clearly presented",
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "No clear results section identified"
          ],
          "detailed_scores": {
            "novelty": 5.5,
            "methodology": 7.9,
            "clarity": 8.8,
            "reproducibility": 6.1
          },
          "recommendations": [
            "Add dedicated results section with clear presentation"
          ],
          "methodology_analysis": {
            "has_methodology_section": false,
            "methodology_length": 0,
            "experimental_design": "unclear"
          },
          "clarity_metrics": {
            "abstract_length": 161,
            "section_count": 15,
            "reference_count": 13,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 13,
          "key_citations": [
            {
              "text": "Alan Akbik, Duncan Blythe, and Roland Vollgraf.",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638\u20131649.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nG",
              "year": 2018,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntion",
              "year": 2006,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "Ronan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning.\nIn Pro-\nceedings of the 25th international conference on\nMa",
              "year": 2008,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\nDan Hendrycks and Kevin Gimpel. 2016.\nBridging\nnonlinearities and stochastic regularizers with gaus-\nsian error lin",
              "year": 2016,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of wo",
              "year": 2013,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "Matthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL.\nMatthew Peters, Mark Neumann, Mohit Iyyer, ",
              "year": 2017,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "U-net:\nMachine reading comprehension\nwith unanswerable questions.\narXiv preprint\narXiv:1810.06638.\nWilson L Taylor. 1953.\nCloze procedure:\nA new\ntool for measuring readability. Journalism Bulletin,\n30",
              "year": 1953,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general met",
              "year": 2003,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "Question:\nDoes BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            }
          ],
          "related_papers": [
            {
              "arxiv_id": "2104.04167v2",
              "title": "The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation",
              "authors": [
                "Yuankai Qi",
                "Zizheng Pan",
                "Yicong Hong"
              ],
              "abstract": "Vision-and-Language Navigation (VLN) requires an agent to find a path to a remote location on the basis of natural-language instructions and a set of photo-realistic panoramas. Most existing methods take the words in the instructions and the discrete views of each panorama as the minimal unit of enc",
              "published": "2021-04-09T02:44:39+00:00",
              "similarity_score": 0.17647058823529413,
              "url": "http://arxiv.org/abs/2104.04167v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1910.03806v1",
              "title": "Is Multilingual BERT Fluent in Language Generation?",
              "authors": [
                "Samuel R\u00f6nnqvist",
                "Jenna Kanerva",
                "Tapio Salakoski"
              ],
              "abstract": "The multilingual BERT model is trained on 104 languages and meant to serve as a universal language model and tool for encoding sentences. We explore how well the model performs on several languages across several tasks: a diagnostic classification probing the embeddings for a particular syntactic pr",
              "published": "2019-10-09T06:35:59+00:00",
              "similarity_score": 0.17647058823529413,
              "url": "http://arxiv.org/abs/1910.03806v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2103.16110v3",
              "title": "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain",
              "authors": [
                "Mingchen Zhuge",
                "Dehong Gao",
                "Deng-Ping Fan"
              ],
              "abstract": "We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT, which introduces a novel kaleido strategy for fashion cross-modality representations from transformers. In contrast to random masking strategy of recent VL models, we design alignment guided masking to jointly focus more o",
              "published": "2021-03-30T06:53:00+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2103.16110v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2503.21676v2",
              "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
              "authors": [
                "Nicolas Zucchet",
                "J\u00f6rg Bornschein",
                "Stephanie Chan"
              ],
              "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn i",
              "published": "2025-03-27T16:43:45+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2503.21676v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2106.11740v2",
              "title": "LV-BERT: Exploiting Layer Variety for BERT",
              "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Fei Chen"
              ],
              "abstract": "Modern pre-trained language models are mostly built upon backbones stacking self-attention and feed-forward layers in an interleaved order. In this paper, beyond this stereotyped layer pattern, we aim to improve pre-trained models by exploiting layer variety from two aspects: the layer type set and ",
              "published": "2021-06-22T13:20:14+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2106.11740v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2203.00328v3",
              "title": "BERT-LID: Leveraging BERT to Improve Spoken Language Identification",
              "authors": [
                "Yuting Nie",
                "Junhong Zhao",
                "Wei-Qiang Zhang"
              ],
              "abstract": "Language identification is the task of automatically determining the identity of a language conveyed by a spoken segment. It has a profound impact on the multilingual interoperability of an intelligent speech system. Despite language identification attaining high accuracy on medium or long utterance",
              "published": "2022-03-01T10:01:25+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2203.00328v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1909.10351v5",
              "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
              "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang"
              ],
              "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate i",
              "published": "2019-09-23T13:05:35+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1909.10351v5",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2307.02054v3",
              "title": "Emoji Prediction in Tweets using BERT",
              "authors": [
                "Muhammad Osama Nusrat",
                "Zeeshan Habib",
                "Mehreen Alam"
              ],
              "abstract": "In recent years, the use of emojis in social media has increased dramatically, making them an important element in understanding online communication. However, predicting the meaning of emojis in a given text is a challenging task due to their ambiguous nature. In this study, we propose a transforme",
              "published": "2023-07-05T06:38:52+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2307.02054v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1610.00031v1",
              "title": "Discriminating Similar Languages: Evaluations and Explorations",
              "authors": [
                "Cyril Goutte",
                "Serge L\u00e9ger",
                "Shervin Malmasi"
              ],
              "abstract": "We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate",
              "published": "2016-09-30T20:57:52+00:00",
              "similarity_score": 0.0,
              "url": "http://arxiv.org/abs/1610.00031v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1501.03191v1",
              "title": "Annotating Cognates and Etymological Origin in Turkic Languages",
              "authors": [
                "Benjamin S. Mericli",
                "Michael Bloodgood"
              ],
              "abstract": "Turkic languages exhibit extensive and diverse etymological relationships among lexical items. These relationships make the Turkic languages promising for exploring automated translation lexicon induction by leveraging cognate and other etymological information. However, due to the extent and divers",
              "published": "2015-01-13T22:14:57+00:00",
              "similarity_score": 0.0,
              "url": "http://arxiv.org/abs/1501.03191v1",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "The paper cites 13 references, indicating moderate engagement with prior research. 0 references are from recent work (2020+), with opportunity to include more recent work.",
          "citation_network": {
            "total_citations": 13,
            "related_papers_found": 10,
            "network_density": 0.7692307692307693,
            "avg_similarity": 0.09611283109735122
          },
          "topics": [
            "Machine Learning",
            "Natural Language"
          ]
        },
        "final_review": {
          "executive_summary": "**{jacobdevlin,mingweichang,kentonl,kristout}@google.com**\n\n**Quick Assessment**: Overall Score 7.1/10\n\n**Main Contribution**: We introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers.\n\n**Verdict**: This paper presents a strong contribution to the field.",
          "detailed_review": "# Detailed Review: {jacobdevlin,mingweichang,kentonl,kristout}@google.com\n\n## Overview\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create sta...\n\n## Assessment Scores\n- **Novelty**: 5.5/10\n- **Methodology**: 7.9/10\n- **Clarity**: 8.8/10\n- **Reproducibility**: 6.1/10\n\n## Strengths\n1. Strong methodology: well-executed and clearly presented\n2. Strong clarity: well-executed and clearly presented\n3. Well-structured paper with detailed sections\n4. Detailed abstract providing good overview\n\n## Weaknesses\n1. No clear results section identified\n\n## Recommendations for Improvement\n1. Add dedicated results section with clear presentation\n\n## Related Work\n- The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation (Similarity: 0.18)\n- Is Multilingual BERT Fluent in Language Generation? (Similarity: 0.18)\n- Kaleido-BERT: Vision-Language Pre-training on Fashion Domain (Similarity: 0.11)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. {jacobdevlin,mingweichang,kentonl,kristout}@google.com is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work is important because it shows a new way to tackle this problem that works really well!\n\n**The bottom line:**\nThe main good thing: Strong methodology: well-executed and clearly presented\nSomething to improve: No clear results section identified",
          "key_takeaways": [
            "Main focus: {jacobdevlin,mingweichang,kentonl,kristout}@google.com",
            "Key strength: Strong methodology: well-executed and clearly presented",
            "Strongest aspect: clarity (8.8/10)",
            "Area for improvement: No clear results section identified",
            "Overall quality: 7.1/10 - Good"
          ],
          "recommendation": "Weak Accept - Good work but needs improvements",
          "confidence": 0.7699999999999999,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                5.5,
                7.9,
                8.8,
                6.1
              ]
            },
            "summary_card": {
              "title": "{jacobdevlin,mingweichang,kentonl,kristout}@google.com",
              "overall_score": 7.075,
              "recommendation": "Weak Accept - Good work but needs improvements"
            },
            "metrics": {
              "sections": 15,
              "references": 13,
              "word_count": 10152
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 4.71295,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:24:11.174300",
          "end_time": "2025-11-25T22:24:15.887250"
        }
      },
      "constraint_violations": [],
      "expectations_met": false,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 7.075
      }
    },
    {
      "test_id": "test_5",
      "name": "Short Paper (Workshop)",
      "timestamp": "2025-11-25T22:24:15.888055",
      "passed": true,
      "processing_time": 3.045263,
      "review_result": {
        "session_id": "1831902a-9ef8-4379-a682-5cd0bc07f65d",
        "paper_content": {
          "paper_id": "paper-providedproperattrib-3ce77ba5",
          "title": "Provided proper attribution is provided, Google hereby grants permission to",
          "authors": [
            "Attention Is All You Need",
            "Ashish Vaswani\u2217",
            "Google Brain",
            "Noam Shazeer\u2217",
            "Google Brain",
            "Niki Parmar\u2217",
            "Google Research",
            "Jakob Uszkoreit\u2217",
            "Google Research",
            "Llion Jones\u2217"
          ],
          "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n1",
          "sections": [
            {
              "heading": "1\nIntroduction",
              "level": 1,
              "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks "
            },
            {
              "heading": "2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU",
              "level": 1,
              "content": "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehens"
            },
            {
              "heading": "3\nModel Architecture",
              "level": 1,
              "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forw"
            },
            {
              "heading": "Attention Is All You Need",
              "level": 2,
              "content": "Ashish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com"
            },
            {
              "heading": "Abstract",
              "level": 2,
              "content": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transf"
            },
            {
              "heading": "Introduction",
              "level": 2,
              "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks "
            },
            {
              "heading": "Background",
              "level": 2,
              "content": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the seque"
            },
            {
              "heading": "Model Architecture",
              "level": 2,
              "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1"
            },
            {
              "heading": "Encoder and Decoder Stacks",
              "level": 2,
              "content": "Encoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We a"
            },
            {
              "heading": "Attention",
              "level": 2,
              "content": "An attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are "
            }
          ],
          "references": [],
          "metadata": {
            "source": "data/sample_papers/workshop_paper.pdf",
            "page_count": 4,
            "word_count": 1707,
            "extracted_at": "2025-11-25T22:24:15.932069"
          }
        },
        "critic_analysis": {
          "overall_score": 5.925,
          "strengths": [
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "Weak reproducibility: needs improvement",
            "Limited references - needs more literature review",
            "No clear results section identified"
          ],
          "detailed_scores": {
            "novelty": 7.0,
            "methodology": 5.0,
            "clarity": 6.9,
            "reproducibility": 4.8
          },
          "recommendations": [
            "Expand literature review to include more recent work",
            "Add dedicated results section with clear presentation",
            "Include implementation details and code availability"
          ],
          "methodology_analysis": {
            "has_methodology_section": false,
            "methodology_length": 0,
            "experimental_design": "unclear"
          },
          "clarity_metrics": {
            "abstract_length": 323,
            "section_count": 10,
            "reference_count": 0,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 0,
          "key_citations": [],
          "related_papers": [
            {
              "arxiv_id": "2106.07104v1",
              "title": "The link between Bitcoin and Google Trends attention",
              "authors": [
                "Nektarios Aslanidis",
                "Aurelio F. Bariviera",
                "\u00d3scar G. L\u00f3pez"
              ],
              "abstract": "This paper shows that Bitcoin is not correlated to a general uncertainty index as measured by the Google Trends data of Castelnuovo and Tran (2017). Instead, Bitcoin is linked to a Google Trends attention measure specific for the cryptocurrency market. First, we find a bidirectional relationship bet",
              "published": "2021-06-13T22:41:55+00:00",
              "similarity_score": 0.1411111111111111,
              "url": "http://arxiv.org/abs/2106.07104v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "2210.09377v1",
              "title": "6th Place Solution to Google Universal Image Embedding",
              "authors": [
                "S. Gkelios",
                "A. Kastellos",
                "S. Chatzichristofis"
              ],
              "abstract": "This paper presents the 6th place solution to the Google Universal Image Embedding competition on Kaggle. Our approach is based on the CLIP architecture, a powerful pre-trained model used to learn visual representation from natural language supervision. We also utilized the SubCenter ArcFace loss wi",
              "published": "2022-10-17T19:19:46+00:00",
              "similarity_score": 0.11263157894736842,
              "url": "http://arxiv.org/abs/2210.09377v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1803.10916v1",
              "title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
              "authors": [
                "Changhao Shan",
                "Junbo Zhang",
                "Yujun Wang"
              ],
              "abstract": "In this paper, we propose an attention-based end-to-end neural approach for small-footprint keyword spotting (KWS), which aims to simplify the pipelines of building a production-quality KWS system. Our model consists of an encoder and an attention mechanism. The encoder transforms the input signal i",
              "published": "2018-03-29T03:32:59+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1803.10916v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1902.08746v1",
              "title": "Can Google Scholar and Mendeley help to assess the scholarly impacts of dissertations?",
              "authors": [
                "Kayvan Kousha",
                "Mike Thelwall"
              ],
              "abstract": "Dissertations can be the single most important scholarly outputs of junior researchers. Whilst sets of journal articles are often evaluated with the help of citation counts from the Web of Science or Scopus, these do not index dissertations and so their impact is hard to assess. In response, this ar",
              "published": "2019-02-23T06:35:12+00:00",
              "similarity_score": 0.09878542510121457,
              "url": "http://arxiv.org/abs/1902.08746v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1301.1290v1",
              "title": "Moving dunes on the Google Earth",
              "authors": [
                "Amelia Carolina Sparavigna"
              ],
              "abstract": "Several methods exist for surveying the dunes and estimate their migration rate. Among methods suitable for the macroscopic scale, the use of the satellite images available on Google Earth is a convenient resource, in particular because of its time series. Some examples of the use of this feature of",
              "published": "2013-01-04T18:31:43+00:00",
              "similarity_score": 0.08263157894736842,
              "url": "http://arxiv.org/abs/1301.1290v1",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1212.0638v2",
              "title": "Manipulating Google Scholar Citations and Google Scholar Metrics: simple, easy and tempting",
              "authors": [
                "Emilio Delgado Lopez-Cozar",
                "Nicolas Robinson-Garcia",
                "Daniel Torres-Salinas"
              ],
              "abstract": "The launch of Google Scholar Citations and Google Scholar Metrics may provoke a revolution in the research evaluation field as it places within every researchers reach tools that allow bibliometric measuring. In order to alert the research community over how easily one can manipulate the data and bi",
              "published": "2012-12-04T08:30:13+00:00",
              "similarity_score": 0.08263157894736842,
              "url": "http://arxiv.org/abs/1212.0638v2",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "1003.3242v3",
              "title": "Private Information Disclosure from Web Searches. (The case of Google Web History)",
              "authors": [
                "Claude Castelluccia",
                "Emiliano De Cristofaro",
                "Daniele Perito"
              ],
              "abstract": "As the amount of personal information stored at remote service providers increases, so does the danger of data theft. When connections to remote services are made in the clear and authenticated sessions are kept using HTTP cookies, data theft becomes extremely easy to achieve. In this paper, we stud",
              "published": "2010-03-16T20:23:59+00:00",
              "similarity_score": 0.07990430622009569,
              "url": "http://arxiv.org/abs/1003.3242v3",
              "reason": "Shared focus on: google"
            },
            {
              "arxiv_id": "2310.10159v1",
              "title": "Joint Music and Language Attention Models for Zero-shot Music Tagging",
              "authors": [
                "Xingjian Du",
                "Zhesong Yu",
                "Jiaju Lin"
              ],
              "abstract": "Music tagging is a task to predict the tags of music recordings. However, previous music tagging research primarily focuses on close-set music tagging tasks which can not be generalized to new tags. In this work, we propose a zero-shot music tagging system modeled by a joint music and language atten",
              "published": "2023-10-16T08:00:16+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2310.10159v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2210.12753v3",
              "title": "Google's Quantum Supremacy Claim: Data, Documentation, and Discussion",
              "authors": [
                "Gil Kalai",
                "Yosef Rinott",
                "Tomer Shoham"
              ],
              "abstract": "In October 2019, Nature published a paper describing an experiment that took place at Google. The paper claims to demonstrate quantum (computational) supremacy on a 53-qubit quantum computer. Since September 2019 we have been involved in a long-term project to study various statistical aspects of th",
              "published": "2022-10-23T15:36:25+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2210.12753v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2009.10633v1",
              "title": "ThingML+ Augmenting Model-Driven Software Engineering for the Internet of Things with Machine Learning",
              "authors": [
                "Armin Moin",
                "Stephan R\u00f6ssler",
                "Stephan G\u00fcnnemann"
              ],
              "abstract": "In this paper, we present the current position of the research project ML-Quadrat, which aims to extend the methodology, modeling language and tool support of ThingML - an open source modeling tool for IoT/CPS - to address Machine Learning needs for the IoT applications. Currently, ThingML offers a ",
              "published": "2020-09-22T15:45:45+00:00",
              "similarity_score": 0.0,
              "url": "http://arxiv.org/abs/2009.10633v1",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "No references found in the paper.",
          "citation_network": {
            "total_citations": 0,
            "related_papers_found": 10,
            "network_density": 10.0,
            "avg_similarity": 0.08140698482803746
          },
          "topics": [
            "Machine Learning",
            "Computer Vision",
            "Natural Language"
          ]
        },
        "final_review": {
          "executive_summary": "**Provided proper attribution is provided, Google hereby grants permission to**\n\n**Quick Assessment**: Overall Score 5.9/10\n\n**Main Contribution**: The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder.\n\n**Verdict**: This paper presents a moderate contribution to the field.",
          "detailed_review": "# Detailed Review: Provided proper attribution is provided, Google hereby grants permission to\n\n## Overview\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being mor...\n\n## Assessment Scores\n- **Novelty**: 7.0/10\n- **Methodology**: 5.0/10\n- **Clarity**: 6.9/10\n- **Reproducibility**: 4.8/10\n\n## Strengths\n1. Well-structured paper with detailed sections\n2. Detailed abstract providing good overview\n\n## Weaknesses\n1. Weak reproducibility: needs improvement\n2. Limited references - needs more literature review\n3. No clear results section identified\n\n## Recommendations for Improvement\n1. Expand literature review to include more recent work\n2. Add dedicated results section with clear presentation\n3. Include implementation details and code availability\n\n## Related Work\n- The link between Bitcoin and Google Trends attention (Similarity: 0.14)\n- 6th Place Solution to Google Universal Image Embedding (Similarity: 0.11)\n- Attention-based End-to-End Models for Small-Footprint Keyword Spotting (Similarity: 0.11)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. Provided proper attribution is provided, Google hereby grants permission to is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work adds to our understanding of the problem, though there's still room for improvement.\n\n**The bottom line:**\nThe main good thing: Well-structured paper with detailed sections\nSomething to improve: Weak reproducibility: needs improvement",
          "key_takeaways": [
            "Main focus: Provided proper attribution is provided, Google hereby grants permission to",
            "Key strength: Well-structured paper with detailed sections",
            "Strongest aspect: novelty (7.0/10)",
            "Area for improvement: Weak reproducibility: needs improvement",
            "Overall quality: 5.9/10 - Fair"
          ],
          "recommendation": "Major Revisions - Significant improvements needed",
          "confidence": 0.88,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                7.0,
                5.0,
                6.9,
                4.8
              ]
            },
            "summary_card": {
              "title": "Provided proper attribution is provided, Google hereby grants permission to",
              "overall_score": 5.925,
              "recommendation": "Major Revisions - Significant improvements needed"
            },
            "metrics": {
              "sections": 10,
              "references": 0,
              "word_count": 1707
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 3.044737,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:24:15.888074",
          "end_time": "2025-11-25T22:24:18.932811"
        }
      },
      "constraint_violations": [],
      "expectations_met": true,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 5.925
      }
    },
    {
      "test_id": "test_6",
      "name": "Multi-Column Layout",
      "timestamp": "2025-11-25T22:24:18.934244",
      "passed": true,
      "processing_time": 4.105853,
      "review_result": {
        "session_id": "39797f92-b907-4af1-97a1-7e765f792dd9",
        "paper_content": {
          "paper_id": "paper-kahevxiangzvshrenjia-298bec90",
          "title": "{kahe, v-xiangz, v-shren, jiansun}@microsoft.com",
          "authors": [
            "Deep Residual Learning for Image Recognition",
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun",
            "Microsoft Research"
          ],
          "abstract": "Deeper neural networks are more dif\ufb01cult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148\u00d7\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classi\ufb01cation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local-\nization, COCO detection, and COCO segmentation.",
          "sections": [
            {
              "heading": "1. Introduction",
              "level": 2,
              "content": "Deep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classi\ufb01cation [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classi\ufb01ers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41, 44] reveals that network depth is of crucial importance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models,\nwith a depth of sixteen [41] to thirty [16]. Many other non-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/\nand\nhttp://mscoco.org/dataset/#detections-challenge2015.\n0\n1\n2\n3\n4\n5\n6\n0 \n10\n20\niter. (1e4)\ntraining error (%)\n \n \n0\n1\n2\n3\n4\n5\n6\n0\n10\n20\niter. (1e4)\ntest error (%)\n \n \n56-layer\n20-layer\n56-layer\n20-layer\nFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer \u201cplain\u201d n"
            },
            {
              "heading": "2. Related Work",
              "level": 2,
              "content": "Residual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re-\ntrieval and classi\ufb01cation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a \ufb01ner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales. It has been shown\n[3, 45, 46] that these solvers converge much faster than stan-\ndard solvers that ar"
            },
            {
              "heading": "3. Deep Residual Learning",
              "level": 2,
              "content": "3.1. Residual Learning\nLet us consider H(x) as an underlying mapping to be\n\ufb01t by a few stacked layers (not necessarily the entire net),\nwith x denoting the inputs to the \ufb01rst of these layers. If one\nhypothesizes that multiple nonlinear layers can asymptoti-\ncally approximate complicated functions2, then it is equiv-\nalent to hypothesize that they can asymptotically approxi-\nmate the residual functions, i.e., H(x) \u2212x (assuming that\nthe input and output are of the same dimensions).\nSo\nrather than expect stacked layers to approximate H(x), we\nexplicitly let these layers approximate a residual function\nF(x) := H(x) \u2212x. The original function thus becomes\nF(x)+x. Although both forms should be able to asymptot-\nically approximate the desired functions (as hypothesized),\nthe ease of learning might be different.\nThis reformulation is motivated by the counterintuitive\nphenomena about the degradation problem (Fig. 1, left). As\nwe discussed in the introduction, if the added layers can\nbe construct"
            },
            {
              "heading": "4. Experiments",
              "level": 2,
              "content": "4.1. ImageNet Classi\ufb01cation\nWe evaluate our method on the ImageNet 2012 classi\ufb01-\ncation dataset [36] that consists of 1000 classes. The models\nare trained on the 1.28 million training images, and evalu-\nated on the 50k validation images. We also obtain a \ufb01nal\nresult on the 100k test images, reported by the test server.\nWe evaluate both top-1 and top-5 error rates.\nPlain Networks. We \ufb01rst evaluate 18-layer and 34-layer\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\n18-layer plain net is of a similar form. See Table 1 for de-\ntailed architectures.\nThe results in Table 2 show that the deeper 34-layer plain\nnet has higher validation error than the shallower 18-layer\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\npare their training/validation errors during the training pro-\ncedure. We have observed the degradation problem - the\n4\nlayer name output size\n18-layer\n34-layer\n50-layer\n101-layer\n152-layer\nconv1\n112\u00d7112\n7\u00d77, 64, stride 2\nconv2 x\n56\u00d756\n3\u00d73 max pool, strid"
            },
            {
              "heading": "8\nReferences",
              "level": 1,
              "content": "[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks, 5(2):157\u2013166, 1994.\n[2] C. M. Bishop.\nNeural networks for pattern recognition.\nOxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\n2000.\n[4] K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nIn BMVC, 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303\u2013338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.\n[9] X. Glorot and Y. Bengio. Und"
            },
            {
              "heading": "11\nLOC\nmethod\nLOC\nnetwork\ntesting LOC error\non GT CLS",
              "level": 1,
              "content": "classi\ufb01cation\nnetwork\ntop-5 LOC error\non predicted CLS\nVGG\u2019s [41]\nVGG-16\n1-crop\n33.1 [41]\nRPN\nResNet-101 1-crop\n13.3\nRPN\nResNet-101 dense\n11.7\nRPN\nResNet-101 dense\nResNet-101\n14.4\nRPN+RCNN ResNet-101 dense\nResNet-101\n10.6\nRPN+RCNN\nensemble\ndense\nensemble\n8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of \u201cLOC error on GT class\u201d ([41]), the ground truth\nclass is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing\non a center crop of 224\u00d7224 pixels, \u201cdense\u201d denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015, surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classi\ufb01ers are \ufb01rst adopted for\npredicting the class labels of "
            },
            {
              "heading": "PASCAL VOC",
              "level": 1,
              "content": "Following [7, 32], for the PASCAL VOC 2007 test set,\nwe use the 5k trainval images in VOC 2007 and 16k train-\nval images in VOC 2012 for training (\u201c07+12\u201d). For the\nPASCAL VOC 2012 test set, we use the 10k trainval+test\nimages in VOC 2007 and 16k trainval images in VOC 2012\nfor training (\u201c07++12\u201d). The hyper-parameters for train-\ning Faster R-CNN are the same as in [32]. Table 7 shows\nthe results. ResNet-101 improves the mAP by >3% over\nVGG-16. This gain is solely because of the improved fea-\ntures learned by ResNet."
            },
            {
              "heading": "MS COCO",
              "level": 1,
              "content": "The MS COCO dataset [26] involves 80 object cate-\ngories. We evaluate the PASCAL VOC metric (mAP @\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\n.5:.05:.95). We use the 80k images on the train set for train-\ning and the 40k images on the val set for evaluation. Our\ndetection system for COCO is similar to that for PASCAL\nVOC. We train the COCO models with an 8-GPU imple-\nmentation, and thus the RPN step has a mini-batch size of\n8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is\nnearly as big as mAP@.5\u2019s (6.9%). This suggests "
            },
            {
              "heading": "MS COCO",
              "level": 1,
              "content": "Box re\ufb01nement. Our box re\ufb01nement partially follows the it-\nerative localization in [6]. In Faster R-CNN, the \ufb01nal output\nis a regressed box that is different from its proposal box. So\nfor inference, we pool a new feature from the regressed box\nand obtain a new classi\ufb01cation score and a new regressed\nbox. We combine these 300 new predictions with the orig-\ninal 300 predictions. Non-maximum suppression (NMS) is\napplied on the union set of predicted boxes using an IoU\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\n\ufb01nement improves mAP by about 2 points (Table 9).\nGlobal context.\nWe combine global context in the Fast\nR-CNN step. Given the full-image conv feature map, we\npool a feature by global Spatial Pyramid Pooling [12] (with\na \u201csingle-level\u201d pyramid) which can be implemented as\n\u201cRoI\u201d pooling using the entire image\u2019s bounding box as the\nRoI. This pooled feature is fed into the post-RoI layers to\nobtain a global context feature. This global feature is con-\ncatenated with the o"
            },
            {
              "heading": "PASCAL VOC",
              "level": 1,
              "content": "We revisit the PASCAL VOC dataset based on the above\nmodel. With the single model on the COCO dataset (55.7%\nmAP@.5 in Table 9), we \ufb01ne-tune this model on the PAS-\nCAL VOC sets. The improvements of box re\ufb01nement, con-\ntext, and multi-scale testing are also adopted. By doing so\nval2\ntest\nGoogLeNet [44] (ILSVRC\u201914)\n-\n43.9\nour single model (ILSVRC\u201915)\n60.5\n58.8\nour ensemble (ILSVRC\u201915)\n63.6\n62.1\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\nOur detection system is Faster R-CNN [32] with the improvements\nin Table 9, using ResNet-101.\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\non PASCAL VOC 2012 is 10 points higher than the previ-\nous state-of-the-art result [6].\nImageNet Detection\nThe ImageNet Detection (DET) task involves 200 object\ncategories. The accuracy is evaluated by mAP@.5. Our\nobject detection algorithm for ImageNet DET is the same\nas that for MS COCO in Table 9. The networks are pre-\ntrained on the 1"
            },
            {
              "heading": "LOC",
              "level": 1,
              "content": "method"
            },
            {
              "heading": "LOC",
              "level": 1,
              "content": "network\ntesting LOC error\non GT CLS\nclassi\ufb01cation\nnetwork\ntop-5 LOC error\non predicted CLS\nVGG\u2019s [41]\nVGG-16\n1-crop\n33.1 [41]"
            },
            {
              "heading": "RPN",
              "level": 1,
              "content": "ResNet-101 1-crop\n13.3"
            },
            {
              "heading": "RPN",
              "level": 1,
              "content": "ResNet-101 dense\n11.7"
            },
            {
              "heading": "RPN",
              "level": 1,
              "content": "ResNet-101 dense\nResNet-101\n14.4\nRPN+RCNN ResNet-101 dense\nResNet-101\n10.6\nRPN+RCNN\nensemble\ndense\nensemble\n8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of \u201cLOC error on GT class\u201d ([41]), the ground truth\nclass is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing\non a center crop of 224\u00d7224 pixels, \u201cdense\u201d denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015, surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classi\ufb01ers are \ufb01rst adopted for\npredicting the class labels of an image, and the localiza-\ntion algorithm only accounts for predicting bounding boxes\nbased on the predicted classes. We adopt the \u201cper-class re-\n"
            }
          ],
          "references": [
            "[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks, 5(2):157\u2013166, 1994.",
            "C. M. Bishop.\nNeural networks for pattern recognition.\nOxford\nuniversity press, 1995.",
            "W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,",
            "K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nIn BMVC, 2011.",
            "M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303\u2013338, 2010.",
            "S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.",
            "R. Girshick. Fast R-CNN. In ICCV, 2015.",
            "R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.",
            "X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training\ndeep feedforward neural networks. In AISTATS, 2010.",
            "I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.",
            "K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR, 2015.",
            "K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV, 2014.",
            "K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. In\nICCV, 2015.",
            "G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580, 2012.",
            "S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735\u20131780, 1997.",
            "S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML, 2015.",
            "H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI, 33, 2011.",
            "H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI, 2012.",
            "Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093, 2014.",
            "A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report, 2009.",
            "A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS, 2012.",
            "Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation, 1989.",
            "Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. Ef\ufb01cient backprop.\nIn Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998.",
            "C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.\nDeeply-\nsupervised nets. arXiv:1409.5185, 2014.",
            "M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400,",
            "T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: Common objects in\ncontext. In ECCV. 2014.",
            "J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. In CVPR, 2015.",
            "G. Mont\u00b4ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep neural networks. In NIPS, 2014.",
            "V. Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted\nboltzmann machines. In ICML, 2010.",
            "F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR, 2007.",
            "T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS, 2012.",
            "S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In NIPS,",
            "S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\nnetworks on convolutional feature maps. arXiv:1504.06066, 2015.",
            "B. D. Ripley. Pattern recognition and neural networks. Cambridge\nuniversity press, 1996.",
            "A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.",
            "O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575, 2014.",
            "A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.\narXiv:1312.6120, 2013.",
            "N. N. Schraudolph. Accelerated gradient descent by factor-centering\ndecomposition. Technical report, 1998.",
            "N. N. Schraudolph. Centering neural network gradient factors. In\nNeural Networks: Tricks of the Trade, pages 207\u2013226. Springer,",
            "P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\nCun.\nOverfeat: Integrated recognition, localization and detection\nusing convolutional networks. In ICLR, 2014.",
            "K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR, 2015.",
            "R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\narXiv:1505.00387, 2015.",
            "R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\nnetworks. 1507.06228, 2015.",
            "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu-\ntions. In CVPR, 2015.",
            "R. Szeliski. Fast surface interpolation using hierarchical basis func-\ntions. TPAMI, 1990.",
            "R. Szeliski. Locally adapted hierarchical basis preconditioning. In\nSIGGRAPH, 2006.",
            "T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas-\ntic gradient towards second-order methods\u2013backpropagation learn-\ning with transformations in nonlinearities.\nIn Neural Information\nProcessing, 2013.",
            "A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\nof computer vision algorithms, 2008.",
            "W. Venables and B. Ripley. Modern applied statistics with s-plus.",
            "M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional neural networks. In ECCV, 2014.\n9\nA. Object Detection Baselines\nIn this section we introduce our detection method based\non the baseline Faster R-CNN [32] system. The models are\ninitialized by the ImageNet classi\ufb01cation models, and then\n\ufb01ne-tuned on the object detection data. We have experi-\nmented with ResNet-50/101 at the time of the ILSVRC &\nCOCO 2015 detection competitions.\nUnlike VGG-16 used in [32], our ResNet has no hidden\nfc layers. We adopt the idea of \u201cNetworks on Conv fea-\nture maps\u201d (NoC) [33] to address this issue. We compute\nthe full-image shared conv feature maps using those lay-\ners whose strides on the image are no greater than 16 pixels\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\nlayers in ResNet-101; Table 1). We consider these layers as\nanalogous to the 13 conv layers in VGG-16, and by doing\nso, both ResNet and VGG-16 have conv feature maps of the\nsame total stride (16 pixels). These layers are shared by a\nregion proposal network (RPN, generating 300 proposals)"
          ],
          "metadata": {
            "source": "data/sample_papers/two_column.pdf",
            "page_count": 12,
            "word_count": 9805,
            "extracted_at": "2025-11-25T22:24:19.048254"
          }
        },
        "critic_analysis": {
          "overall_score": 6.825,
          "strengths": [
            "Strong methodology: well-executed and clearly presented",
            "Strong clarity: well-executed and clearly presented",
            "Comprehensive literature review with extensive citations",
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "No clear results section identified"
          ],
          "detailed_scores": {
            "novelty": 5.0,
            "methodology": 7.9,
            "clarity": 8.8,
            "reproducibility": 5.6
          },
          "recommendations": [
            "Add dedicated results section with clear presentation"
          ],
          "methodology_analysis": {
            "has_methodology_section": true,
            "methodology_length": 1000,
            "experimental_design": "present"
          },
          "clarity_metrics": {
            "abstract_length": 206,
            "section_count": 15,
            "reference_count": 50,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 50,
          "key_citations": [
            {
              "text": "[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks, 5(2):157\u2013166, 1994.",
              "year": 1994,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "C. M. Bishop.\nNeural networks for pattern recognition.\nOxford\nuniversity press, 1995.",
              "year": 1995,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,",
              "year": null,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nIn BMVC, 2011.",
              "year": 2011,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303\u2013338, 2010.",
              "year": 2010,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.",
              "year": 2015,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "R. Girshick. Fast R-CNN. In ICCV, 2015.",
              "year": 2015,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.",
              "year": 2014,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training\ndeep feedforward neural networks. In AISTATS, 2010.",
              "year": 2010,
              "relevance": "medium",
              "type": "unknown"
            },
            {
              "text": "I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.",
              "year": 2013,
              "relevance": "medium",
              "type": "unknown"
            }
          ],
          "related_papers": [
            {
              "arxiv_id": "1512.03385v1",
              "title": "Deep Residual Learning for Image Recognition",
              "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren"
              ],
              "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of l",
              "published": "2015-12-10T19:51:55+00:00",
              "similarity_score": 0.6666666666666666,
              "url": "http://arxiv.org/abs/1512.03385v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1905.10944v1",
              "title": "Identity Connections in Residual Nets Improve Noise Stability",
              "authors": [
                "Shuzhi Yu",
                "Carlo Tomasi"
              ],
              "abstract": "Residual Neural Networks (ResNets) achieve state-of-the-art performance in many computer vision problems. Compared to plain networks without residual connections (PlnNets), ResNets train faster, generalize better, and suffer less from the so-called degradation problem. We introduce simplified (but s",
              "published": "2019-05-27T02:52:46+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1905.10944v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2201.12328v2",
              "title": "Toward Training at ImageNet Scale with Differential Privacy",
              "authors": [
                "Alexey Kurakin",
                "Shuang Song",
                "Steve Chien"
              ],
              "abstract": "Differential privacy (DP) is the de facto standard for training machine learning (ML) models, including neural networks, while ensuring the privacy of individual examples in the training set. Despite a rich literature on how to train ML models with differential privacy, it remains extremely challeng",
              "published": "2022-01-28T18:48:18+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2201.12328v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2405.16582v1",
              "title": "A comparison of the Coco-Russo scheme and $\\protect\\mathghost$-FEM for elliptic equations in arbitrary domains",
              "authors": [
                "Clarissa Astuto",
                "Armando Coco",
                "Umberto Zerbinati"
              ],
              "abstract": "In this paper, a comparative study between the Coco-Russo scheme (based on finite-difference scheme) and the $\\mathghost$-FEM (based on finite-element method) is presented when solving the Poisson equation in arbitrary domains. The comparison between the two numerical methods is carried out by prese",
              "published": "2024-05-26T14:26:52+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2405.16582v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2403.18819v1",
              "title": "Benchmarking Object Detectors with COCO: A New Path Forward",
              "authors": [
                "Shweta Singh",
                "Aayan Yadav",
                "Jitesh Jain"
              ],
              "abstract": "The Common Objects in Context (COCO) dataset has been instrumental in benchmarking object detectors over the past decade. Like every dataset, COCO contains subtle errors and imperfections stemming from its annotation procedure. With the advent of high-performing models, we ask whether these errors o",
              "published": "2024-03-27T17:59:53+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2403.18819v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2006.07159v1",
              "title": "Are we done with ImageNet?",
              "authors": [
                "Lucas Beyer",
                "Olivier J. H\u00e9naff",
                "Alexander Kolesnikov"
              ],
              "abstract": "Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for colle",
              "published": "2020-06-12T13:17:25+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2006.07159v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2111.01956v1",
              "title": "One Pass ImageNet",
              "authors": [
                "Huiyi Hu",
                "Ang Li",
                "Daniele Calandriello"
              ],
              "abstract": "We present the One Pass ImageNet (OPIN) problem, which aims to study the effectiveness of deep learning in a streaming setting. ImageNet is a widely known benchmark dataset that has helped drive and evaluate recent advancements in deep learning. Typically, deep learning methods are trained on static",
              "published": "2021-11-03T00:28:45+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2111.01956v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2106.03658v2",
              "title": "Reduction Using Induced Subnets To Systematically Prove Properties For Free-Choice Nets",
              "authors": [
                "Wil M. P. van der Aalst"
              ],
              "abstract": "We use sequences of t-induced T-nets and p-induced P-nets to convert free-choice nets into T-nets and P-nets while preserving properties such as well-formedness, liveness, lucency, pc-safety, and perpetuality. The approach is general and can be applied to different properties. This allows for more s",
              "published": "2021-06-07T14:37:39+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2106.03658v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2205.04596v2",
              "title": "When does dough become a bagel? Analyzing the remaining mistakes on ImageNet",
              "authors": [
                "Vijay Vasudevan",
                "Benjamin Caine",
                "Raphael Gontijo-Lopes"
              ],
              "abstract": "Image classification accuracy on the ImageNet dataset has been a barometer for progress in computer vision over the last decade. Several recent papers have questioned the degree to which the benchmark remains useful to the community, yet innovations continue to contribute gains to performance, with ",
              "published": "2022-05-09T23:25:45+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2205.04596v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2311.09215v3",
              "title": "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy",
              "authors": [
                "Kirill Vishniakov",
                "Zhiqiang Shen",
                "Zhuang Liu"
              ],
              "abstract": "Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However,",
              "published": "2023-11-15T18:56:51+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2311.09215v3",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "The paper cites 50 references, indicating a comprehensive literature review. 0 references are from recent work (2020+), with opportunity to include more recent work.",
          "citation_network": {
            "total_citations": 50,
            "related_papers_found": 10,
            "network_density": 0.2,
            "avg_similarity": 0.12573099415204678
          },
          "topics": [
            "Machine Learning",
            "Computer Vision",
            "Data Science"
          ]
        },
        "final_review": {
          "executive_summary": "**{kahe, v-xiangz, v-shren, jiansun}@microsoft.com**\n\n**Quick Assessment**: Overall Score 6.8/10\n\n**Main Contribution**: Deeper neural networks are more dif\ufb01cult to train.\n\n**Verdict**: This paper presents a moderate contribution to the field.",
          "detailed_review": "# Detailed Review: {kahe, v-xiangz, v-shren, jiansun}@microsoft.com\n\n## Overview\nDeeper neural networks are more dif\ufb01cult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. O...\n\n## Assessment Scores\n- **Novelty**: 5.0/10\n- **Methodology**: 7.9/10\n- **Clarity**: 8.8/10\n- **Reproducibility**: 5.6/10\n\n## Strengths\n1. Strong methodology: well-executed and clearly presented\n2. Strong clarity: well-executed and clearly presented\n3. Comprehensive literature review with extensive citations\n4. Well-structured paper with detailed sections\n5. Detailed abstract providing good overview\n\n## Weaknesses\n1. No clear results section identified\n\n## Recommendations for Improvement\n1. Add dedicated results section with clear presentation\n\n## Related Work\n- Deep Residual Learning for Image Recognition (Similarity: 0.67)\n- Identity Connections in Residual Nets Improve Noise Stability (Similarity: 0.11)\n- Toward Training at ImageNet Scale with Differential Privacy (Similarity: 0.11)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. {kahe, v-xiangz, v-shren, jiansun}@microsoft.com is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work adds to our understanding of the problem, though there's still room for improvement.\n\n**The bottom line:**\nThe main good thing: Strong methodology: well-executed and clearly presented\nSomething to improve: No clear results section identified",
          "key_takeaways": [
            "Main focus: {kahe, v-xiangz, v-shren, jiansun}@microsoft.com",
            "Key strength: Strong methodology: well-executed and clearly presented",
            "Strongest aspect: clarity (8.8/10)",
            "Area for improvement: No clear results section identified",
            "Overall quality: 6.8/10 - Good"
          ],
          "recommendation": "Weak Accept - Good work but needs improvements",
          "confidence": 0.7199999999999999,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                5.0,
                7.9,
                8.8,
                5.6
              ]
            },
            "summary_card": {
              "title": "{kahe, v-xiangz, v-shren, jiansun}@microsoft.com",
              "overall_score": 6.825,
              "recommendation": "Weak Accept - Good work but needs improvements"
            },
            "metrics": {
              "sections": 15,
              "references": 50,
              "word_count": 9805
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 4.105311,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:24:18.934265",
          "end_time": "2025-11-25T22:24:23.039576"
        }
      },
      "constraint_violations": [],
      "expectations_met": true,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 6.825
      }
    },
    {
      "test_id": "test_7",
      "name": "Error Handling - Invalid PDF",
      "timestamp": "2025-11-25T22:24:23.040822",
      "passed": true,
      "processing_time": 0.048894,
      "review_result": {
        "session_id": "e97d2914-8e3c-4a3e-931b-5a608443556c",
        "paper_content": {},
        "critic_analysis": {
          "overall_score": 4.75,
          "strengths": [
            "Clear research question"
          ],
          "weaknesses": [
            "Weak reproducibility: needs improvement",
            "Limited references - needs more literature review",
            "Limited section structure - could be more detailed",
            "No clear results section identified"
          ],
          "detailed_scores": {
            "novelty": 5.0,
            "methodology": 5.0,
            "clarity": 5.0,
            "reproducibility": 4.0
          },
          "recommendations": [
            "Expand literature review to include more recent work",
            "Add dedicated results section with clear presentation",
            "Include implementation details and code availability"
          ],
          "methodology_analysis": {
            "has_methodology_section": false,
            "methodology_length": 0,
            "experimental_design": "unclear"
          },
          "clarity_metrics": {
            "abstract_length": 0,
            "section_count": 0,
            "reference_count": 0,
            "has_clear_structure": false
          }
        },
        "citation_data": {
          "citation_count": 0,
          "key_citations": [],
          "related_papers": [],
          "citation_context": "No references found in the paper.",
          "citation_network": {
            "total_citations": 0,
            "related_papers_found": 0,
            "network_density": 0.0,
            "avg_similarity": 0.0
          },
          "topics": []
        },
        "final_review": {
          "executive_summary": "**Unknown Title**\n\n**Quick Assessment**: Overall Score 4.8/10\n\n**Main Contribution**: No abstract available.\n\n**Verdict**: This paper presents a weak contribution to the field.",
          "detailed_review": "# Detailed Review: Unknown Title\n\n## Overview\n\n\n## Assessment Scores\n- **Novelty**: 5.0/10\n- **Methodology**: 5.0/10\n- **Clarity**: 5.0/10\n- **Reproducibility**: 4.0/10\n\n## Strengths\n1. Clear research question\n\n## Weaknesses\n1. Weak reproducibility: needs improvement\n2. Limited references - needs more literature review\n3. Limited section structure - could be more detailed\n4. No clear results section identified\n\n## Recommendations for Improvement\n1. Expand literature review to include more recent work\n2. Add dedicated results section with clear presentation\n3. Include implementation details and code availability\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. This paper is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work explores an interesting idea, but needs more development to be really useful.\n\n**The bottom line:**\nThe main good thing: Clear research question\nSomething to improve: Weak reproducibility: needs improvement",
          "key_takeaways": [
            "Key strength: Clear research question",
            "Strongest aspect: novelty (5.0/10)",
            "Area for improvement: Weak reproducibility: needs improvement",
            "Overall quality: 4.8/10 - Fair"
          ],
          "recommendation": "Major Revisions - Significant improvements needed",
          "confidence": 1.0,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                5.0,
                5.0,
                5.0,
                4.0
              ]
            },
            "summary_card": {
              "title": "Unknown",
              "overall_score": 4.75,
              "recommendation": "Major Revisions - Significant improvements needed"
            },
            "metrics": {
              "sections": 0,
              "references": 0,
              "word_count": 0
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 0.048478,
          "tool_calls": 4,
          "errors": [
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'.",
            "Reader Agent error: Failed to open file 'data/sample_papers/corrupted.pdf'."
          ],
          "start_time": "2025-11-25T22:24:23.040855",
          "end_time": "2025-11-25T22:24:23.089333"
        }
      },
      "constraint_violations": [],
      "expectations_met": true,
      "metrics": {
        "tool_calls": 4,
        "errors": 8,
        "overall_score": 4.75
      }
    },
    {
      "test_id": "test_8",
      "name": "End-to-End Performance",
      "timestamp": "2025-11-25T22:24:23.090066",
      "passed": true,
      "processing_time": 4.650915,
      "review_result": {
        "session_id": "f87f05c9-a018-479e-aa93-676e4781e921",
        "paper_content": {
          "paper_id": "paper-jimmypsiutorontoca-4fe52e21",
          "title": "jimmy@psi.utoronto.ca",
          "authors": [
            "Jimmy Lei Ba\u2217"
          ],
          "abstract": "We introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1",
          "sections": [
            {
              "heading": "1\nINTRODUCTION",
              "level": 1,
              "content": "Stochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning "
            },
            {
              "heading": "2\nALGORITHM",
              "level": 1,
              "content": "See algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient"
            },
            {
              "heading": "3\nINITIALIZATION BIAS CORRECTION",
              "level": 1,
              "content": "As explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true "
            },
            {
              "heading": "4\nCONVERGENCE ANALYSIS",
              "level": 1,
              "content": "We analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that cont"
            },
            {
              "heading": "5\nRELATED WORK",
              "level": 1,
              "content": "Optimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s precondi"
            },
            {
              "heading": "6\nEXPERIMENTS",
              "level": 1,
              "content": "To empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with o"
            },
            {
              "heading": "7\nEXTENSIONS",
              "level": 1,
              "content": "7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the"
            },
            {
              "heading": "8\nCONCLUSION",
              "level": 1,
              "content": "We have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning."
            },
            {
              "heading": "9\nACKNOWLEDGMENTS",
              "level": 1,
              "content": "This paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Res"
            },
            {
              "heading": "10\nAPPENDIX",
              "level": 1,
              "content": "10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe bas"
            },
            {
              "heading": "ABSTRACT",
              "level": 1,
              "content": "We introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably "
            },
            {
              "heading": "INTRODUCTION",
              "level": 1,
              "content": "Stochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning "
            },
            {
              "heading": "ALGORITHM",
              "level": 1,
              "content": "See algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient"
            },
            {
              "heading": "INITIALIZATION BIAS CORRECTION",
              "level": 1,
              "content": "As explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true "
            },
            {
              "heading": "CONVERGENCE ANALYSIS",
              "level": 1,
              "content": "We analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) ="
            }
          ],
          "references": [
            "Amari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,",
            "Sohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX",
            "1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15"
          ],
          "metadata": {
            "source": "data/sample_papers/standard_paper.pdf",
            "page_count": 15,
            "word_count": 7136,
            "extracted_at": "2025-11-25T22:24:23.168441"
          }
        },
        "critic_analysis": {
          "overall_score": 6.75,
          "strengths": [
            "Strong methodology: well-executed and clearly presented",
            "Strong clarity: well-executed and clearly presented",
            "Well-structured paper with detailed sections",
            "Detailed abstract providing good overview"
          ],
          "weaknesses": [
            "Limited references - needs more literature review",
            "No clear results section identified"
          ],
          "detailed_scores": {
            "novelty": 5.0,
            "methodology": 7.6,
            "clarity": 8.8,
            "reproducibility": 5.6
          },
          "recommendations": [
            "Expand literature review to include more recent work",
            "Add dedicated results section with clear presentation"
          ],
          "methodology_analysis": {
            "has_methodology_section": false,
            "methodology_length": 0,
            "experimental_design": "unclear"
          },
          "clarity_metrics": {
            "abstract_length": 162,
            "section_count": 15,
            "reference_count": 3,
            "has_clear_structure": true
          }
        },
        "citation_data": {
          "citation_count": 3,
          "key_citations": [
            {
              "text": "Amari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael,",
              "year": 1998,
              "relevance": "high",
              "type": "journal"
            },
            {
              "text": "Sohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on",
              "year": 2014,
              "relevance": "medium",
              "type": "journal"
            },
            {
              "text": "1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded ",
              "year": 2015,
              "relevance": "medium",
              "type": "unknown"
            }
          ],
          "related_papers": [
            {
              "arxiv_id": "1809.09284v1",
              "title": "Tree-Based Optimization: A Meta-Algorithm for Metaheuristic Optimization",
              "authors": [
                "Benyamin Ghojogh",
                "Saeed Sharifian",
                "Hoda Mohammadzade"
              ],
              "abstract": "Designing search algorithms for finding global optima is one of the most active research fields, recently. These algorithms consist of two main categories, i.e., classic mathematical and metaheuristic algorithms. This article proposes a meta-algorithm, Tree-Based Optimization (TBO), which uses other",
              "published": "2018-09-25T02:19:24+00:00",
              "similarity_score": 0.17647058823529413,
              "url": "http://arxiv.org/abs/1809.09284v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2105.13646v3",
              "title": "Conic-Optimization Based Algorithms for Nonnegative Matrix Factorization",
              "authors": [
                "Valentin Leplat",
                "Yurii Nesterov",
                "Nicolas Gillis"
              ],
              "abstract": "Nonnegative matrix factorization is the following problem: given a nonnegative input matrix $V$ and a factorization rank $K$, compute two nonnegative matrices, $W$ with $K$ columns and $H$ with $K$ rows, such that $WH$ approximates $V$ as well as possible. In this paper, we propose two new approache",
              "published": "2021-05-28T07:48:56+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2105.13646v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2302.12021v2",
              "title": "Derivative-Free Optimization with Transformed Objective Functions (DFOTO) and the Algorithm Based on the Least Frobenius Norm Updating Quadratic Model",
              "authors": [
                "Pengcheng Xie",
                "Ya-xiang Yuan"
              ],
              "abstract": "Derivative-free optimization problems are optimization problems where derivative information is unavailable. The least Frobenius norm updating quadratic interpolation model function is one of the essential under-determined model functions for model-based derivative-free trust-region methods. This ar",
              "published": "2023-02-19T15:42:50+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2302.12021v2",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1808.06100v6",
              "title": "On the solution existence and stability of polynomial optimization problems",
              "authors": [
                "Vu Trung Hieu"
              ],
              "abstract": "This paper introduces and investigates a regularity condition in the asymptotic sense for optimization problems whose objective functions are polynomial. Under this regularity condition, the normalization argument in asymptotic analysis enables us to see the solution existence as well as the solutio",
              "published": "2018-08-18T16:27:40+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1808.06100v6",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2508.02719v1",
              "title": "ZetA: A Riemann Zeta-Scaled Extension of Adam for Deep Learning",
              "authors": [
                "Samiksha BC"
              ],
              "abstract": "This work introduces ZetA, a novel deep learning optimizer that extends Adam by incorporating dynamic scaling based on the Riemann zeta function. To the best of our knowledge, ZetA is the first optimizer to apply zeta-based gradient scaling within deep learning optimization. The method improves gene",
              "published": "2025-08-01T02:53:29+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/2508.02719v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1607.01655v3",
              "title": "L1 penalization of volumetric dose objectives in optimal control of PDEs",
              "authors": [
                "Richard C. Barnard",
                "Christian Clason"
              ],
              "abstract": "This work is concerned with a class of optimal control problems governed by a partial differential equation that are motivated by an application in radiotherapy treatment planning, where the primary design objective is to minimize the volume where a functional of the state violates a prescribed leve",
              "published": "2016-07-06T15:03:32+00:00",
              "similarity_score": 0.1111111111111111,
              "url": "http://arxiv.org/abs/1607.01655v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2501.00258v1",
              "title": "Optimal design of frame structures with mixed categorical and continuous design variables using the Gumbel-Softmax method",
              "authors": [
                "Mehran Ebrahimi",
                "Hyunmin Cheong",
                "Pradeep Kumar Jayaraman"
              ],
              "abstract": "In optimizing real-world structures, due to fabrication or budgetary restraints, the design variables may be restricted to a set of standard engineering choices. Such variables, commonly called categorical variables, are discrete and unordered in essence, precluding the utilization of gradient-based",
              "published": "2024-12-31T03:59:24+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2501.00258v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "1912.08495v1",
              "title": "Provable Non-Convex Optimization and Algorithm Validation via Submodularity",
              "authors": [
                "Yatao An Bian"
              ],
              "abstract": "Submodularity is one of the most well-studied properties of problem classes in combinatorial optimization and many applications of machine learning and data mining, with strong implications for guaranteed optimization. In this thesis, we investigate the role of submodularity in provable non-convex o",
              "published": "2019-12-18T10:13:38+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/1912.08495v1",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2201.02491v3",
              "title": "An efficient and easy-to-extend Matlab code of the Moving Morphable Component (MMC) method for three-dimensional topology optimization",
              "authors": [
                "Zongliang Du",
                "Tianchen Cui",
                "Chang Liu"
              ],
              "abstract": "Explicit topology optimization methods have received ever-increasing interest in recent years. In particular, a 188-line Matlab code of the two-dimensional (2D) Moving Morphable Component (MMC)-based topology optimization method was released by Zhang et al. (Struct Multidiscip Optim 53(6):1243-1260,",
              "published": "2022-01-07T15:10:53+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2201.02491v3",
              "reason": "Related methodology or domain"
            },
            {
              "arxiv_id": "2303.08710v1",
              "title": "Two-Scale Optimization of Graded Lattice Structures respecting Buckling on Micro- and Macroscale",
              "authors": [
                "Daniel H\u00fcbner",
                "Fabian Wein",
                "Michael Stingl"
              ],
              "abstract": "Interest in components with detailed structures increased with the progress in advanced manufacturing techniques in recent years. Parts with graded lattice elements can provide interesting mechanical, thermal, and acoustic properties compared to parts where only coarse features are included. One of ",
              "published": "2023-03-15T15:44:31+00:00",
              "similarity_score": 0.05263157894736842,
              "url": "http://arxiv.org/abs/2303.08710v1",
              "reason": "Related methodology or domain"
            }
          ],
          "citation_context": "The paper cites 3 references, indicating limited reference to prior work. 0 references are from recent work (2020+), with opportunity to include more recent work.",
          "citation_network": {
            "total_citations": 3,
            "related_papers_found": 10,
            "network_density": 3.3333333333333335,
            "avg_similarity": 0.09425524595803234
          },
          "topics": [
            "Data Science",
            "Optimization"
          ]
        },
        "final_review": {
          "executive_summary": "**jimmy@psi.utoronto.ca**\n\n**Quick Assessment**: Overall Score 6.8/10\n\n**Main Contribution**: We introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments.\n\n**Verdict**: This paper presents a moderate contribution to the field.",
          "detailed_review": "# Detailed Review: jimmy@psi.utoronto.ca\n\n## Overview\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/o...\n\n## Assessment Scores\n- **Novelty**: 5.0/10\n- **Methodology**: 7.6/10\n- **Clarity**: 8.8/10\n- **Reproducibility**: 5.6/10\n\n## Strengths\n1. Strong methodology: well-executed and clearly presented\n2. Strong clarity: well-executed and clearly presented\n3. Well-structured paper with detailed sections\n4. Detailed abstract providing good overview\n\n## Weaknesses\n1. Limited references - needs more literature review\n2. No clear results section identified\n\n## Recommendations for Improvement\n1. Expand literature review to include more recent work\n2. Add dedicated results section with clear presentation\n\n## Related Work\n- Tree-Based Optimization: A Meta-Algorithm for Metaheuristic Optimization (Similarity: 0.18)\n- Conic-Optimization Based Algorithms for Nonnegative Matrix Factorization (Similarity: 0.11)\n- Derivative-Free Optimization with Transformed Objective Functions (DFOTO) and the Algorithm Based on the Least Frobenius Norm Updating Quadratic Model (Similarity: 0.11)\n",
          "eli5_summary": "**What is this paper about?**\nImagine you're trying to solve a puzzle. jimmy@psi.utoronto.ca is like finding a new way to put the pieces together.\n\n**What did they do?**\nThe researchers looked at a problem and tried a new approach to solve it. They tested their idea and checked if it worked better than previous methods.\n\n**Why does it matter?**\nThis work adds to our understanding of the problem, though there's still room for improvement.\n\n**The bottom line:**\nThe main good thing: Strong methodology: well-executed and clearly presented\nSomething to improve: Limited references - needs more literature review",
          "key_takeaways": [
            "Main focus: jimmy@psi.utoronto.ca",
            "Key strength: Strong methodology: well-executed and clearly presented",
            "Strongest aspect: clarity (8.8/10)",
            "Area for improvement: Limited references - needs more literature review",
            "Overall quality: 6.8/10 - Good"
          ],
          "recommendation": "Weak Accept - Good work but needs improvements",
          "confidence": 0.7199999999999999,
          "visual_elements": {
            "score_chart": {
              "categories": [
                "novelty",
                "methodology",
                "clarity",
                "reproducibility"
              ],
              "values": [
                5.0,
                7.6,
                8.8,
                5.6
              ]
            },
            "summary_card": {
              "title": "jimmy@psi.utoronto.ca",
              "overall_score": 6.75,
              "recommendation": "Weak Accept - Good work but needs improvements"
            },
            "metrics": {
              "sections": 15,
              "references": 3,
              "word_count": 7136
            }
          }
        },
        "metadata": {
          "processing_time_seconds": 4.650474,
          "tool_calls": 4,
          "errors": [],
          "start_time": "2025-11-25T22:24:23.090085",
          "end_time": "2025-11-25T22:24:27.740559"
        }
      },
      "constraint_violations": [],
      "expectations_met": true,
      "metrics": {
        "tool_calls": 4,
        "errors": 0,
        "overall_score": 6.75
      }
    }
  ],
  "aggregate_metrics": {
    "success_rate": 62.5,
    "avg_latency": 3.495659,
    "median_latency": 4.2988265,
    "total_tool_calls": 32,
    "avg_tool_calls": 4.0,
    "total_constraint_violations": 0,
    "error_rate": 12.5,
    "avg_overall_score": 6.4625,
    "latency_distribution": {
      "min": 0.048894,
      "max": 4.713354,
      "p50": 4.2988265,
      "p90": 4.6696467,
      "p95": 4.69150035,
      "p99": 4.70898327
    },
    "failure_breakdown": {
      "total_failures": 3,
      "failure_reasons": {},
      "failure_rate_by_test": {
        "test_1": [],
        "test_2": [],
        "test_4": []
      }
    }
  },
  "summary": {
    "total_tests": 8,
    "passed": 5,
    "failed": 3,
    "success_rate": 62.5
  }
}